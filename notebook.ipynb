{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false,
    "language": "python",
    "name": "cell1",
    "resultHeight": 0,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.core import Root, CreateMode\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.core.schema import Schema\n",
    "from snowflake.core.stage import Stage\n",
    "from snowflake.core.table import Table, TableColumn, PrimaryKey\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "from snowflake.core.warehouse import Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully\n"
     ]
    }
   ],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "connection_parameters = {\n",
    "        \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "        \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "       \n",
    "    }\n",
    "\n",
    "\n",
    "try:\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    root = Root(session)\n",
    "    print(\"Connected successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during connection: {e}\")\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created database\n"
     ]
    }
   ],
   "source": [
    "import snowflake.core.database\n",
    "\n",
    "\n",
    "    \n",
    "database = root.databases.create(\n",
    "    Database(\n",
    "        name =\"CORTEX_CONNECT_DB\"),\n",
    "        mode=CreateMode.or_replace\n",
    "    )\n",
    "print(\"created database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = database.schemas.create(\n",
    "    Schema(\n",
    "        name=\"CORTEX_SEARCH_SCHEMA\"),\n",
    "        mode = CreateMode.or_replace,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable is NOT a Pandas DataFrame.\n",
      "Formatted Chunks of Text:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "https://doi.org/10.1007/s11042-022-12136-3\n",
      "AhybridCTC+Attentionmodelbasedonend-to-end\n",
      "frameworkformultilingualspeechrecognition\n",
      "SendongLiang1 ·WeiQiYan 1\n",
      "Received:16May2021/Revised:2August2021/Accepted:3January2022 /\n",
      "© TheAuthor(s)2022\n",
      "Abstract\n",
      "Speech recognition is an important field in natural language processing. In this paper, the\n",
      "end-to-end framework for speech recognition with multilingual datasets is proposed. The\n",
      "end-to-end methods do not require complicated alignment and construction of the pronun-\n",
      "ciation dictionary, which show a promising prospect. In this paper, we implement a hybrid\n",
      "model of CTC and attention (CTC+Attention) model based on PyTorch. In order to compare\n",
      "speech recognition methods for multiple languages, we design and create three datasets:\n",
      "Chinese, English, and Code-Switch. We evaluate the proposed hybrid CTC+Attention\n",
      "model in multilingual environment. Throughout our experiments, we find that the proposed\n",
      "hybrid CTC+Attention model based on end-to-end framework achieves better performance\n",
      "compared with the HMM-DNN model in a single language and Code-Switch speaking envi-\n",
      "ronment. Moreover, the results of speech recognition with regard to different languages\n",
      "are compared in this paper. The CER(i.e., Character Error Rate) of the proposed hybrid\n",
      "CTC+Attention model based on the Chinese dataset defeated the traditional model and\n",
      "reached 10.22%.\n",
      "Keywords Speech recognition · End-to-end framework · Attention model · CTC model ·\n",
      "Code-Switch\n",
      "1 Introduction\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "CTC+Attention model based on the Chinese dataset defeated the traditional model and\n",
      "reached 10.22%.\n",
      "Keywords Speech recognition · End-to-end framework · Attention model · CTC model ·\n",
      "Code-Switch\n",
      "1 Introduction\n",
      "Spoken language is essential to modern human cultures. Speech is a way of social com-\n",
      "munications amongst people through languages. In the past decades, with the development\n",
      "of intelligent devices, the developed applications began to enter people’s daily life. While\n",
      "/envelopebackWei Qi Yan\n",
      "weiqi.yan@aut.ac.nz\n",
      "Sendong Liang\n",
      "ghg0412@autuni.ac.nz\n",
      "1 School of Engineering, Computer & Mathematics, Auckland University of Technology,\n",
      "No. 31 Symonds Street, Auckland 1010, New Zealand\n",
      "Published online: 20 May 2022\n",
      "Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "people communicate with intelligent devices through languages, automatic speech recog-\n",
      "nition(ASR) plays pivotal role. ASR aims to convert audio data into corresponding text,\n",
      "the text is further processed through human-computer interaction, such as multilingual\n",
      "translations and hand talk, etc.\n",
      "In the early stage, because it was impossible to directly model the audio-to-text conver-\n",
      "sion, Bayes’ theorem was implemented to convert human speech into text so as to calculate\n",
      "the corresponding audio features. Accordingly, the probability of audio feature sequences is\n",
      "decomposed to the product of conditional probabilities of corresponding audio features.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "the corresponding audio features. Accordingly, the probability of audio feature sequences is\n",
      "decomposed to the product of conditional probabilities of corresponding audio features.\n",
      "With the development of HMM, speech recognition has transited from seperated words\n",
      "of a small system for speech recognition to a large vocabulary continuous system nowa-\n",
      "days [40]. The framework based on HMM model for speech recognition shows its\n",
      "excellence and reliable stability, which was the mainstream speech recognition model.\n",
      "Classic HMM speech recognition model assumes that the model state transferring has\n",
      "the homogeneous Markov property. The HMM needs to be trained based on a set of speech\n",
      "sequences and requires a larger dataset. It is said that smaller models are easier to under-\n",
      "stand, but larger models can fit the data [31]. By considering the non-stationary process with\n",
      "the range of frequency and time of the speech signals, HMM models have weak robustness\n",
      "performance because they focus on the temporal analysis.\n",
      "Deep neural network has contributed to acoustic models and formed the HMM-DNN\n",
      "speech recognition framework [ 18]. The output of objective function of the DNN-based\n",
      "acoustic model is the probability of a HMM state given a sequence of audio features. With\n",
      "more and more research work dedicated to deep learning models, the ability of acoustic\n",
      "models becomes stronger and stronger. A classification network has been directly created\n",
      "from the HMM-DNN acoustic model [8].\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "more and more research work dedicated to deep learning models, the ability of acoustic\n",
      "models becomes stronger and stronger. A classification network has been directly created\n",
      "from the HMM-DNN acoustic model [8].\n",
      "Owing to the soaring deep learning, the end-to-end frameworks for speech recognition\n",
      "have shown exemplary performance in high-resource languages. However, it is hard to per-\n",
      "form well in low-resource datasets for speech recognition, such as Chinese-English and\n",
      "Code-Switch environment [41]. In this paper, we will devote to bridge the gap and provide\n",
      "our solution for resolving this problem.\n",
      "Up to date, our daily communications often are surrounded by mixed languages, which\n",
      "is academically named as Code-Switch model. For example, a lot of Chinese people mingle\n",
      "English words in a Chinese sentence. The speech blended with the words from multilin-\n",
      "gual is one of the critical challenges in speech recognition. The main technical difficulty\n",
      "also includes the non-native accents, the composition brings difficulties to model the mixed\n",
      "acoustics, meanwhile, the labelled datasets for the mixed speech recognition are extremely\n",
      "scarce.\n",
      "Traditional phonetic framework is based on basic acoustic unit for language recognition.\n",
      "The linguistic information is various for different languages, such as Chinese phonetic con-\n",
      "sonants and English phonemes. The framework relies on specific linguistic knowledge and\n",
      "is tough to be expanded to multilingual speech recognition. The end-to-end frameworks\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 5 ---\n",
      "\n",
      "sonants and English phonemes. The framework relies on specific linguistic knowledge and\n",
      "is tough to be expanded to multilingual speech recognition. The end-to-end frameworks\n",
      "employ a unified network for modelling and are dependent more on datasets than linguis-\n",
      "tic information. Accordingly, we have a great interest in using the end-to-end framework to\n",
      "resolve this emerging speech recognition problem.\n",
      "The focus of this paper is on speech recognition using our labeled datasets and mul-\n",
      "tilanguage environment in the end-to-end framework. On the basis of related work, four\n",
      "experiments will be implemented with the datasets and multiple models. The corresponding\n",
      "results will also be analysed. The contributions of this paper include: (1) Investigating the\n",
      "performance of a hybrid model for speech recognition based on our labelled datasets; (2)\n",
      "Exploring an end-to-end framework for speech recognition with multi-language datasets;\n",
      "41296 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "(3) Comparing the end-to-end framework based on hybrid CTC+Attention model with the\n",
      "traditional speech recognition model. Moreover, the experimental results will be compared\n",
      "and analysed.\n",
      "The structure of the proposed CTC+Attention hybrid model is split into three main parts.\n",
      "The first part is pre-net, which is composed of Deep CNN inspired by using the VGG\n",
      "structure. The second part is the encoder shared by CTC and Attention. The last part is a\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 6 ---\n",
      "\n",
      "The first part is pre-net, which is composed of Deep CNN inspired by using the VGG\n",
      "structure. The second part is the encoder shared by CTC and Attention. The last part is a\n",
      "joint decoder, which is composed of a CTC decoder, an attention decoder, and a language\n",
      "model. The architecture of the CTC/Attention hybrid model is shown in Fig.1.\n",
      "Fig.1 The structure of hybrid model\n",
      "41297Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "The remaining part of this paper is organized as follows. We have our literature review in\n",
      "Section 2, our method is presented in Section 3, our result analysis is shown in Section 4,\n",
      "our conclusion is drawn in Section 5.\n",
      "2 Literaturereview\n",
      "Speech recognition models are grouped into three categories. The first category includes\n",
      "rule-based models, such as Shoebox and Harpy created by IBM and CMU in 1962 and\n",
      "1976, respectively [11]. The second group encompasses statistical models such as Large\n",
      "V ocabulary Continuous Speech Recognition (LVCSR) and HMM. HMM-Gaussian Mix-\n",
      "ture Model (GMM) has been the dominant framework in the field of speech recognition\n",
      "till the emerge of deep learning [37]. The third one is deep-learning-based models such as\n",
      "Deep Neural Networks (DNNs), Convolutional Neural Networks (CNNs), Recurrent Neural\n",
      "Networks (RNNs), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BiL-\n",
      "STM) [26,32]. LSTM is sensitive to the static data, which may lead to delays with respect\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 7 ---\n",
      "\n",
      "Deep Neural Networks (DNNs), Convolutional Neural Networks (CNNs), Recurrent Neural\n",
      "Networks (RNNs), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BiL-\n",
      "STM) [26,32]. LSTM is sensitive to the static data, which may lead to delays with respect\n",
      "to features, BiLSTM appears as a special architecture operating the input sequence in both\n",
      "directions [17].\n",
      "In 2006, an unsupervised method was employed to pretrain the Deep Belief Networks\n",
      "(DBNs), which solved the problem that gradient descent is sensitive to the initial value\n",
      "[9]. DNNs have been applied to acoustic modelling to form the HMM-DNN framework\n",
      "in speech recognition [18, 32]. The objective function of DNN-based acoustic models is\n",
      "the probability of a HMM state given a sequence of audio features. In 2009, DNNs were\n",
      "applied to the TIMIT phoneme recognition and achieved excellent performance [23]. In\n",
      "2012, CNNs were applied to the LVCSR, which normalized the data and obtained a higher\n",
      "performance in speech recognition [1].\n",
      "Moreover, an end-to-end framework for speech recognition based on RNNs and\n",
      "Weighted Finite State Transducer (WFST) decoding method was proposed in 2015. This\n",
      "end-to-end framework directly utilises analogue signals as its input, which improves the\n",
      "recognition rate [21] instantaneously. In 2017, a new method of end-to-end speech recog-\n",
      "nition was proffered, which employs CTC model in a multitask framework to improve\n",
      "robustness of the system and achieves quick convergence, therefore alleviates the problem\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 8 ---\n",
      "\n",
      "nition was proffered, which employs CTC model in a multitask framework to improve\n",
      "robustness of the system and achieves quick convergence, therefore alleviates the problem\n",
      "of data alignment [12].\n",
      "However, most of the previous research work in speech recognition focuses on better\n",
      "performance of speech recognition for a single language or a single task. Pertaining to\n",
      "comparison of speech recognition between multiple languages, the publications of speech\n",
      "recognition in a multilingual or Code-Switch environment are relatively rare.\n",
      "Feedforward Neural Networks (FFNNs) are dependent on the preceding words, which\n",
      "cannot learn the dependent information of long sentences. With development of deep learn-\n",
      "ing, RNNs turn up, which aims at solving the issue of long sequence dependence. RNNs are\n",
      "defined as a type of ANNs that make classification and predictions for various data, such as\n",
      "text, audio, video, genomes, etc. [19].\n",
      "Compared with traditional FFNNs, such as multilayer perception, using static classi-\n",
      "fiers and only considering fixed-size input windows which are irrespective of surrounding\n",
      "context, RNNs are more effective and suitable to transcribe time series such as speech tran-\n",
      "scription because of its hidden network layers [6]. Different from FFNNs using fixed-length\n",
      "context, RNNs do not take use of a limited size of context, which contain cache models\n",
      "41298 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 9 ---\n",
      "\n",
      "scription because of its hidden network layers [6]. Different from FFNNs using fixed-length\n",
      "context, RNNs do not take use of a limited size of context, which contain cache models\n",
      "41298 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "that encode temporal information implicitly with arbitrary lengths [ 22]. The recurrent con-\n",
      "nections allow information to cycle inside networks for a long time adapting to the past\n",
      "inputs [2]. A continuous vector space best suits for word representation, meanwhile, deep\n",
      "learning methods reflect the relationship between continuous words. Accordingly, com-\n",
      "pared with FFNNs, RNNs best solve the contextual dependency problem which spans over\n",
      "a fixed number of predecessor words [33].\n",
      "The end-to-end framework for speech recognition refers to directly transduce the input\n",
      "sequence of acoustic feature vectors to the output sequence of token such as phonemes,\n",
      "characters, or words [14]. The end-to-end model is split into three categories based on\n",
      "the alignment methods: Connectionist Temporal Classification (CTC), Attention Encoder-\n",
      "Decoder (AED), and RNN Transducer (RNN-T), which have been widely utilized in\n",
      "large-scale speech recognition [14]. The end-to-end models are more suitable for on-device\n",
      "applications than conventional speech recognition because there are fewer parameters by\n",
      "folding the acoustics, pronunciation, and language models into one neural network [13].\n",
      "Attention-based encoder-decoder model such as LAS (Listen, Attend, and Spell) con-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 10 ---\n",
      "\n",
      "folding the acoustics, pronunciation, and language models into one neural network [13].\n",
      "Attention-based encoder-decoder model such as LAS (Listen, Attend, and Spell) con-\n",
      "tains three main components: Encoder as an acoustic model, attender as an alignment model,\n",
      "and decoder as a language model [4], which subsumes the components of acoustics, pro-\n",
      "nunciation and language models into a single neural network without a lexicon or a text\n",
      "normalization component [5].\n",
      "Applied to Google voice search, the proposed model achieves a WER of 5.6%, while\n",
      "the hybrid HMM-LSTM model attains 6.7% WER. Throughout testing the same models\n",
      "based on dictation, the proposed model reaches up to 4.1%, the HMM-LSTM model gets\n",
      "5% WER. The decoding process of sequence-to-sequence (S2S) models with soft atten-\n",
      "tion incurs a quadratic time cost, which is regarded as a challenge for online sequence\n",
      "transduction [5].\n",
      "In order to address the online streaming challenge of the attention-based model,\n",
      "monotonic-chunkwise attention was put forward, which splits an input sequence into a\n",
      "number of small chunks [5]. Triggered attention equipped with the CTC-based classifier\n",
      "performs well to control the activation of the attention-based decoder [24].\n",
      "3 Methods\n",
      "3.1 Datapreparation\n",
      "3.1.1 Languagefeatures\n",
      "By considering English and Mandarin are worldwide widely used languages in the low-\n",
      "resource environment of bilingual Code-Switch corpus, it is important to select appropriate\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 11 ---\n",
      "\n",
      "3 Methods\n",
      "3.1 Datapreparation\n",
      "3.1.1 Languagefeatures\n",
      "By considering English and Mandarin are worldwide widely used languages in the low-\n",
      "resource environment of bilingual Code-Switch corpus, it is important to select appropriate\n",
      "speech units for acoustic modelling, which convert a speech unit to a corresponding feature\n",
      "vector sequence [30].\n",
      "English is an Indo-European language, while Mandarin is a Sino-Tibetan language [ 3].\n",
      "Based on the Oxford Dictionary, English is written in Latin alphabets, namely, the Roman\n",
      "alphabets, which contains 26 letters and nearly 170,000 words. The general modelling units\n",
      "in English include phone, subword, and character [38].\n",
      "In the field of Chinese speech recognition, there are various available acoustic modelling\n",
      "units, including Chinese characters (word), syllable (syllable), semi-syllable (initial/final),\n",
      "phoneme (phone), which is generally based on phonetic knowledge or data-driven genera-\n",
      "tion [43]. Mandarin contains more than 6,000 characters, 60 phonemes, 408 atonal syllables,\n",
      "41299Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "and 1,302 toned syllables [ 16]. Each syllable includes initials, finals and tones. In total,\n",
      "Chinese (Mandarin) has 22 initials and 39 finals of syllables.\n",
      "Meanwhile, there are a plenty of homophones and polyphones in Chinese [ 44], which\n",
      "may need high-level non-acoustic context knowledge for speech recognition. A small flex-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 12 ---\n",
      "\n",
      "Chinese (Mandarin) has 22 initials and 39 finals of syllables.\n",
      "Meanwhile, there are a plenty of homophones and polyphones in Chinese [ 44], which\n",
      "may need high-level non-acoustic context knowledge for speech recognition. A small flex-\n",
      "ible unit may lead to the difficulties in calibrating the training dataset. By contrast, the\n",
      "limitation of flexibility as well as the high requirement of lexicon and out of vocab-\n",
      "ulary (OOV) [35] are significant challenges albeit the large unit with high recognition\n",
      "performance. The syllable unit meets the requirement and flexibility [29]. Moreover, the\n",
      "utilization of syllables with tones effectively increases the recognition accuracy compared\n",
      "with Chinese characters and syllable initial/final with tones [7].\n",
      "3.2 Corpusdesign\n",
      "Based on the language features in Section 3.1.1, three speech datasets are applied to our\n",
      "experiments, which include dataset Alpha (Mandarin), dataset Beta (English), and dataset\n",
      "Gamma (Mandarin-English).\n",
      "We create the three datasets through the text-to-speech iFLYTEK InterPhonic toolkit that\n",
      "is a software developed by iFLYTEK [10], which converts text into male or female voices.\n",
      "The toolkit is based on an advanced large corpus and a phonetic prosody description, the\n",
      "quality of the synthesized voices in .wavformat files is comparable to that of a real person.\n",
      "Converting the transcript text to synthesised acoustic speeches controls the variables. For\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 13 ---\n",
      "\n",
      "quality of the synthesized voices in .wavformat files is comparable to that of a real person.\n",
      "Converting the transcript text to synthesised acoustic speeches controls the variables. For\n",
      "example, speaker’s accents, emotions, and environmental noises are consistent. The focus\n",
      "of our experiments by using the synthesised acoustic audio is on the recognition rates within\n",
      "the multilingual environment without considering useless variables or factors.\n",
      "3.2.1 Datasetalpha(mandarin)\n",
      "Alpha is a Mandarin speech dataset. We created this dataset through the iFlytek toolkit based\n",
      "on the transcripts of the THCHS-30 corpus. In the THCHS-30 corpus, there are numer-\n",
      "ous transcript files corresponding to the acoustic files. Each file contains three rows, which\n",
      "is the correctly-labeled transcript of the corresponding acoustic sentence. The first row is\n",
      "the Chinese characters of the corresponding acoustic sentence. The second row is the Chi-\n",
      "nese Pinyin with a tone, which corresponds to the Chinese characters. The third row is the\n",
      "syllable initial-final tone.\n",
      "A ss h o w ni nF i g .2, we selected 8,000 files of the THCHS-30 corpus. The first row\n",
      "(Chinese characters) of each selected file are collected as the input of the iFlytek InterPhonic\n",
      "Fig.2 The process of creating alpha-train\n",
      "41300 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "toolkit. The selected Chinese characters are synthesised into 8,000 files sampled at 16 kHz.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 14 ---\n",
      "\n",
      "Fig.2 The process of creating alpha-train\n",
      "41300 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "toolkit. The selected Chinese characters are synthesised into 8,000 files sampled at 16 kHz.\n",
      "The second row (Chinese Pinyin with tones) of each selected file is collected together as\n",
      "one text file as our labeled transcript for training. Throughout this operation process, we\n",
      "created the training subset based on Alpha (Mandarin).\n",
      "Similarly, we created the “Dev” (development) subset and the “Test” subset from Alpha\n",
      "(Mandarin) through the same process. The “Dev” subset is synthesised from other 1,000\n",
      "files of the THCHS-30 corpus except for the selected 8,000 files. The “Test” subset is\n",
      "synthesised from other 1,000 transcript files except for the selected 9,000 files.\n",
      "The synthesised acoustic files in the dataset Alpha (i.e., Mandarin) are split into three\n",
      "groups as shown in Table 1: One-hour acoustic files as the test subset (i.e., Alpha-Test);\n",
      "One-hour acoustic files as the development subset (Alpha-Dev) to train the rapid reaction\n",
      "and performance evaluation, the other 10-hour acoustic files as the training subset (i.e.,\n",
      "Alpha-Train).\n",
      "3.2.2 Datasetbeta(english)\n",
      "Dataset Beta (English) is an English speech dataset. We created this dataset by using the\n",
      "iFlytek InterPhonic toolkit based on the transcript files of the LibriSpeech corpus [25]. The\n",
      "production method is similar to Alpha as shown in Fig.3. The structure of the Beta dataset is\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 15 ---\n",
      "\n",
      "iFlytek InterPhonic toolkit based on the transcript files of the LibriSpeech corpus [25]. The\n",
      "production method is similar to Alpha as shown in Fig.3. The structure of the Beta dataset is\n",
      "shown in Table2: 10-hour acoustic files synthesised as the training subset (i.e., Beta-Train),\n",
      "one-hour audio files as the development subset (i.e., Beta-Dev) to train the rapid reaction\n",
      "and performance evaluation, and one-hour audio files as the testing subset (i.e., Beta-Test).\n",
      "3.2.3 DatasetGamma(mandarin-english)\n",
      "Dataset Gamma (Mandarin-English) is a mixed Mandarin-English bilingual speech\n",
      "database. We created this dataset through the iFlytek InterPhonic toolkit based on the tran-\n",
      "script files of the TAL-CSASR courpus [34]. The original TAL-CSASR corpus is the audio\n",
      "captured in English class teaching environment. The production method and folder structure\n",
      "of the Gamma dataset are similar to Alpha and Beta. The production method is shown in\n",
      "Fig.4. The folder structure of the Gamma dataset is as same as Alpha and Beta. The details\n",
      "of each sub-dataset are shown in Table 3.\n",
      "3.3 Ourexperiments\n",
      "3.3.1 Experimentenvironmentandsetup\n",
      "We implement Kaldi [28] in the experiment of traditional speech recognition. The end to end\n",
      "framework takes use of PyTorch and ESPnet [39] frameworks based on LAS [4] in the RNN\n",
      "language model. Based on the inspiration [ 36], we superimposed mutilhead and location\n",
      "Table1 The statistics of dataset alpha (mandarin)\n",
      "Datasets Time(Hours) Speakers Male Female Percentages of THCHS-30\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 16 ---\n",
      "\n",
      "language model. Based on the inspiration [ 36], we superimposed mutilhead and location\n",
      "Table1 The statistics of dataset alpha (mandarin)\n",
      "Datasets Time(Hours) Speakers Male Female Percentages of THCHS-30\n",
      "Alpha-Train 10 2 1 1 29.878%\n",
      "Alpha-Dev 1 2 1 1 2.988%\n",
      "Alpha-Test 1 2 1 1 2.988%\n",
      "41301Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "Fig.3 The process of creating dataset beta-train\n",
      "Table2 The dataset beta (english)\n",
      "Dataset Time(hour) Speakers Male Female Percentage of LibriSpeech-clean\n",
      "Train 10 2 1 1 8.977%\n",
      "Test 1 2 1 1 0.898%\n",
      "Dev 1 2 1 1 0.898%\n",
      "Fig.4 The process of creating dataset gamma\n",
      "Table3 The dataset gamma (mandarin-english)\n",
      "Datasets Time(hours) Speakers Male Female\n",
      "Train 10 2 1 1\n",
      "Dev 1 2 1 1\n",
      "Test 1 2 1 1\n",
      "41302 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "attention in the hybrid framework. Table 4 shows the details of hardware configuration and\n",
      "operating system.\n",
      "3.3.2 Evaluationmethods\n",
      "The most straightforward approach to evaluate the performance of speech recognition is to\n",
      "calculate the word error rate (WER) [20]. WER is employed for the English dataset, which\n",
      "is computed through (1).\n",
      "R = IE + DE + SE\n",
      "N × 100% (1)\n",
      "where IE refers to the insertion number of English words, DE means the deletion number\n",
      "of English words, SE stands for the substitution number of English words, and N is the\n",
      "total number of English words in the correct sentence. However, characters are considered\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 17 ---\n",
      "\n",
      "of English words, SE stands for the substitution number of English words, and N is the\n",
      "total number of English words in the correct sentence. However, characters are considered\n",
      "instead of words in the Chinese THCHS-30 corpus. Similarly, the character error rate (CER)\n",
      "is calculated through (2), in which the suffixes ended withM take the place of E.\n",
      "R = IM + DM + SM\n",
      "N × 100% (2)\n",
      "3.3.3 Parameteroptimization\n",
      "The main idea of the CTC+Attention hybrid model is to utilise CTC to force the alignment\n",
      "of the eigenvectors of the audio frames to reduce a single tag [27]. At the same time, CTC\n",
      "does not allow skipping label output under the same audio characteristics to avoid the frame\n",
      "skipping mentioned in the attention subsection. The score function of the hybrid model is\n",
      "described by using (3) based on the schematic diagram of the combination of CTC and\n",
      "attention formulas. Among them, Ohybrid is the prediction result of this model, Y is the\n",
      "text label sequence, X is the feature vector sequence corresponding to the audio frame, λ\n",
      "is the evaluation weight of the CTC model. log Pc(Y | X) is the score function of CTC,\n",
      "and log Pa(Y | X) is the score function of the attention model. Therefore, the optimization\n",
      "process is regarded as finding the optimal solution of λ.\n",
      "Ohybrid = (1 − λ) log Pa(Y | X) + λlog Pc(Y | X) (3)\n",
      "Five model trainings were carried out for λ from 0.2 to 0.7 based on Alpha dataset. The\n",
      "losses with different λ are shown in Fig. 5. The red line is the loss of the attention model,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 18 ---\n",
      "\n",
      "Ohybrid = (1 − λ) log Pa(Y | X) + λlog Pc(Y | X) (3)\n",
      "Five model trainings were carried out for λ from 0.2 to 0.7 based on Alpha dataset. The\n",
      "losses with different λ are shown in Fig. 5. The red line is the loss of the attention model,\n",
      "the blue line is the loss of CTC. Judged from the loss of CTC, the convergence speed of\n",
      "the five experiments has been improved. While CTC improves the learning efficiency, the\n",
      "convergence of attention does show relatively large fluctuations. Ifλ equals to 0.7, the loss\n",
      "of attention has a huge fluctuation in a short period.\n",
      "Table4 The hardware equipment of our experiments\n",
      "OS Name Ubuntu20.04\n",
      "OS Type 64-bit\n",
      "Memory 32 GiB\n",
      "Processor Inter(R)Core i9-9900k CPU @ 3.60GHZ x 16\n",
      "GPU GeForce RTX 2080Ti 11GiB x 2\n",
      "Disk Capactiy 1.5T\n",
      "41303Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "Fig.5 The comparisons of loss with various CTC weights\n",
      "On the other hand, the CERs of our five experiments based on the Alpha-Dev subset are\n",
      "listed in Table 5.I fλ is less than 0.6, the CER obtained with the increase of λ becomes\n",
      "smaller, from 8.85% to 8.32%. However, if λ is equal to 0.6, CER starts increasing. Unlike\n",
      "the slight decrease in attention model, the performance of CTC regarding CER dropped\n",
      "from 14.28% to 11.04% as λ on the raise. The minimum is reached if λ is equal to 0.6.\n",
      "Although λ is equal to 0.5, attention model has achieved the best results. Nevertheless,\n",
      "compared with the slight difference in attention, if λ is equal to 0.6, the CTC improvement\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 19 ---\n",
      "\n",
      "Although λ is equal to 0.5, attention model has achieved the best results. Nevertheless,\n",
      "compared with the slight difference in attention, if λ is equal to 0.6, the CTC improvement\n",
      "is much remarkable. Combined the experimental results in Fig. 5 and Table 5, we see that λ\n",
      "equals 0.6 in the mixed model is the optimal solution.\n",
      "41304 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "T\n",
      "able5 The comparisons of\n",
      "CERs based on “Dev” subset\n",
      "with various CTC weights\n",
      "λ CER(Attention) CER(CTC)\n",
      "0.2 8.85% 14.28%\n",
      "0.4 8.52% 12.84%\n",
      "0.5 8.24% 12.02%\n",
      "0.6 8.32% 11.04%\n",
      "0.7 8.51% 11.17%\n",
      "3.4 Experimentalresults\n",
      "The evaluations were carried out based on three datasets: Alpha, Beta, and Gamma,\n",
      "respectively. The final experimental results and the previous three experimental results are\n",
      "compared in Table6.\n",
      "4R e s u l t a n a l y s i s\n",
      "The first row in Table 6 shows that the traditional HMM-based speech recognition model\n",
      "achieved 10.91% error rate in Chinese and 18.23% in English. These two items are much\n",
      "higher than only implementing CTC or attention model in speech recognition. However, the\n",
      "CTC+Attention hybrid model obtained better results based on the Chinese dataset, with the\n",
      "error rate 0.069% less than that of the traditional model. Based on the Code-Switch dataset,\n",
      "the result of the traditional speech recognition model regarding word error is the best of\n",
      "the three models, but the difference with CTC+Attention is not obvious, and the outcome is\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 20 ---\n",
      "\n",
      "the result of the traditional speech recognition model regarding word error is the best of\n",
      "the three models, but the difference with CTC+Attention is not obvious, and the outcome is\n",
      "improved by 3%. The evaluation results of all models in the Gamma dataset are the worst\n",
      "one compared with other two datasets. That is due to the complexity of mixed languages.\n",
      "Moreover, through the experimental results, we see that the accuracy of CTC+Attention\n",
      "hybrid model is the highest one, an error rate 10.22% was achieved based on the Chinese\n",
      "dataset.\n",
      "From the language perspective, the experimental results show that the English dataset\n",
      "is more challenging to be used for model training than the Chinese one. Compared with\n",
      "the three languages, Chinese labels with Pinyin performs better than English in speech\n",
      "recognition. The convergence in the training phase and the WERs in the verification phase\n",
      "consistently reflect that the Chinese dataset Alpha labeled with Pinyin is easier to be trained,\n",
      "the recognition accuracy is greater than English. The utilization of Pinyin effectively avoids\n",
      "the situation of one sound and multiple characters in Chinese.\n",
      "On the other hand, the WER of the HMM-TDNN-F model in the dataset Gamma is\n",
      "25.62%. In contrast, the CTC+Attention model has a 37% gap between the two datasets.\n",
      "The results show that the difference of recognition accuracy of the CTC+Attention model by\n",
      "Table6 The comparisons of experimental results(CERs/WERs)\n",
      "Models Alpha Beta Gamma\n",
      "HMM-TDNN-F 10.91% 18.23% 25.62%\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 21 ---\n",
      "\n",
      "The results show that the difference of recognition accuracy of the CTC+Attention model by\n",
      "Table6 The comparisons of experimental results(CERs/WERs)\n",
      "Models Alpha Beta Gamma\n",
      "HMM-TDNN-F 10.91% 18.23% 25.62%\n",
      "CTC+Attention 10.22% 19.05% 26.11%\n",
      "41305Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "using different language datasets is the smallest one. The difference of multilingual speech\n",
      "recognition is better than the traditional framework.\n",
      "In this paper, our experimental results are analyzed from two aspects. In contrast, CTC+\n",
      "Attention model performs better in the Chinese dataset, while the traditional model per-\n",
      "forms better in English and mixed language. In terms of language, Pinyin labeled datasets\n",
      "are easier to be applied to model training. CTC + Attention has less difference by taking\n",
      "different datasets into account in terms of the accuracy of different language recognition.\n",
      "5 Conclusion\n",
      "Throughout this paper, we provide a research foundation for future exploration of\n",
      "speech recognition with mixed languages. The comparison of actual outcomes in multi-\n",
      "ple languages provides a future research direction based on the characteristics of speech\n",
      "recognition.\n",
      "In this paper, we investigate speech recognition performance based on the CTC and atten-\n",
      "tion hybrid model by using our three labeled datasets. The model attains 10.91%, 18.23%,\n",
      "and 25.62% error rates based on the Chinese, English, and Chinese-English Code-Switch\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 22 ---\n",
      "\n",
      "tion hybrid model by using our three labeled datasets. The model attains 10.91%, 18.23%,\n",
      "and 25.62% error rates based on the Chinese, English, and Chinese-English Code-Switch\n",
      "datasets. The CTC+Attention model adopts the optimal solution to complete the model eval-\n",
      "uations and achieves similar performance to the traditional model. The evaluation based on\n",
      "the Chinese dataset defeated the traditional model and reached 10.22% CER. Albeit the per-\n",
      "formance based on the English and Code-Switch datasets was not as good as the traditional\n",
      "model, the gap remained within 3%.\n",
      "In future, more attention model will be added to our project to replace the current\n",
      "model [42]. We will implement the predetermined algorithms to improve the training results.\n",
      "In addition, we will explore multiple languages for speech recognition by using the methods\n",
      "of creating datasets in this paper [15].\n",
      "Funding Open Access funding enabled and organized by CAUL and its Member Institutions.\n",
      "Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\n",
      "permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\n",
      "appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\n",
      "and indicate if changes were made. The images or other third party material in this article are included in the\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 23 ---\n",
      "\n",
      "appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\n",
      "and indicate if changes were made. The images or other third party material in this article are included in the\n",
      "article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\n",
      "not included in the article’s Creative Commons licence and your intended use is not permitted by statutory\n",
      "regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n",
      "To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n",
      "References\n",
      "1. Abdel-Hamid O, Mohamed AR, Jiang H, Penn G (2012) Applying convolutional neural networks con-\n",
      "cepts to hybrid NN-HMM model for speech recognition. In: IEEE International conference on acoustics,\n",
      "speech and signal processing (ICASSP). IEEE, pp 4277–4280\n",
      "2. Boden M (2002) A guide to recurrent neural networks and backpropagation. The Dallas project: SICS\n",
      "technical report\n",
      "3. Chan JY, Ching P, Lee T, Meng HM (2004) Detection of language boundary in code-switching utter-\n",
      "ances by bi-phone probabilities. In: International symposium on chinese spoken language processing.\n",
      "IEEE, pp. 293–296\n",
      "4. Chan W, Jaitly N, Le QV, Vinyals O (2015) Listen, attend and spell. arXiv: 1508.01211\n",
      "5. Chiu CC, Raffel C (2017) Monotonic chunkwise attention. arXiv: 1712.05382\n",
      "41306 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "6.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 24 ---\n",
      "\n",
      "IEEE, pp. 293–296\n",
      "4. Chan W, Jaitly N, Le QV, Vinyals O (2015) Listen, attend and spell. arXiv: 1508.01211\n",
      "5. Chiu CC, Raffel C (2017) Monotonic chunkwise attention. arXiv: 1712.05382\n",
      "41306 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "6.\n",
      "Eyben F, W ¨ollmer M, Schuller B, Graves A (2009) From speech to letters-using a novel neural net-\n",
      "work architecture for grapheme based ASR. In: IEEE Workshop on automatic speech recognition &\n",
      "understanding. IEEE, pp 376–380\n",
      "7. Fu L, Li X, Zi L (2020)\n",
      "8. Georgescu AL, Cucu H, Burileanu C (2019) Kaldi-based DNN architectures for speech recognition in\n",
      "Romanian. In: International conference on speech technology and human-computer dialogue (sped).\n",
      "IEEE, pp 1–6\n",
      "9. Hinton GE, Osindero S, Teh YW (2006) A fast learning algorithm for deep belief nets. Neural Comput\n",
      "18(7):1527–1554\n",
      "10. iFLYTEK Co., Ltd: Online TTS WebAPI. Website (2020). https://global.xfyun.cn/products/online tts\n",
      "11. Jason CA, Kumar S (2020) An appraisal on speech and emotion recognition technologies based on\n",
      "machine learning. Language 67:68\n",
      "12. Kim S, Hori T, Watanabe S (2017) Joint CTC-attention based end-to-end speech recognition using\n",
      "multi-task learning. In: IEEE International conference on acoustics, speech and signal processing. IEEE,\n",
      "pp 4835–4839\n",
      "13. Li B, Chang Sy, Sainath TN, Pang R, He Y, Strohman T, Wu Y (2020) Towards fast and accurate\n",
      "streaming end-to-end ASR. In: IEEE International conference on acoustics, speech and signal processing\n",
      "(ICASSP). IEEE, pp 6069–6073\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 25 ---\n",
      "\n",
      "pp 4835–4839\n",
      "13. Li B, Chang Sy, Sainath TN, Pang R, He Y, Strohman T, Wu Y (2020) Towards fast and accurate\n",
      "streaming end-to-end ASR. In: IEEE International conference on acoustics, speech and signal processing\n",
      "(ICASSP). IEEE, pp 6069–6073\n",
      "14. Li J, Zhao R, Hu H, Gong Y (2019) Improving RNN transducer modeling for end-to-end speech\n",
      "recognition. In: IEEE Automatic speech recognition and understanding workshop. IEEE, pp 114–121\n",
      "15. Liang S (2021) Multilingual speech recognition based on the end-to-end framework (Master’s Thesis).\n",
      "Auckland University of Technology, New Zealand\n",
      "16. Lin CH, Lee LS, Ting PY (1993) A new framework for recognition of Mandarin syllables with tones\n",
      "using sub-syllabic units. In: IEEE International conference on acoustics, speech, and signal processing,\n",
      "vol. 2. IEEE, pp 227–230\n",
      "17. Liu Z, Chen Q, Hu H, Tang H, Zou Y (2019) Teacher-student learning and post-processing for robust biL-\n",
      "STM mask-based acoustic beamforming. In: International conference on neural information processing.\n",
      "Springer, pp. 522–533\n",
      "18. Maas AL, Qi P, Xie Z, Hannun AY, Lengerich CT, Jurafsky D, Ng AY (2017) Building DNN acoustic\n",
      "models for large vocabulary speech recognition. Comput Speech Lang 41:195–213\n",
      "19. Manaswi NK, Manaswi NK, John S (2018) Deep learning with applications using python. Springer\n",
      "20. Mansikkaniemi A (2010) Acoustic model and language model adaptation for a mobile dictation service\n",
      "(master’s thesis). Aalto university\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 26 ---\n",
      "\n",
      "19. Manaswi NK, Manaswi NK, John S (2018) Deep learning with applications using python. Springer\n",
      "20. Mansikkaniemi A (2010) Acoustic model and language model adaptation for a mobile dictation service\n",
      "(master’s thesis). Aalto university\n",
      "21. Miao Y, Gowayyed M, Metze F (2015) EESEN: End-to-end Speech recognition using deep RNN models\n",
      "and WFST-based decoding. In: IEEE Workshop on automatic speech recognition and understanding\n",
      "(ASRU). IEEE, pp 167–174\n",
      "22. Mikolov T, Karafi ´at M, Burget L, ˇCernock`y J, Khudanpur S (2010) Recurrent neural network based\n",
      "language model. In: Annual conference of the International speech communication association\n",
      "23. Mohamed Ar, Dahl G, Hinton G (2009) Deep belief networks for phone recognition. In: NIPS Workshop\n",
      "on deep learning for speech recognition and related applications, vol. 1. Vancouver, Canada, p 39\n",
      "24. Moritz N, Hori T, Le Roux J (2019) Triggered attention for end-to-end speech recognition. In: IEEE\n",
      "International conference on acoustics, speech and signal processing. IEEE, pp 5666–5670\n",
      "25. Panayotov V, Chen G, Povey D, Khudanpur S (2015) Librispeech: an ASR corpus based on public\n",
      "domain audio books. In: IEEE International conference on acoustics, speech and signal processing.\n",
      "IEEE, pp 5206–5210\n",
      "26. Passricha V, Aggarwal RK (2020) A hybrid of deep cnn and bidirectional LSTM for automatic speech\n",
      "recognition. J Intell Syst 29(1):1261–1274\n",
      "27. Petridis S, Stafylakis T, Ma P, Tzimiropoulos G, Pantic M (2018) Audio-visual speech recognition with a\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 27 ---\n",
      "\n",
      "26. Passricha V, Aggarwal RK (2020) A hybrid of deep cnn and bidirectional LSTM for automatic speech\n",
      "recognition. J Intell Syst 29(1):1261–1274\n",
      "27. Petridis S, Stafylakis T, Ma P, Tzimiropoulos G, Pantic M (2018) Audio-visual speech recognition with a\n",
      "hybrid CTC/attention architecture. In: IEEE Spoken language technology workshop. IEEE, pp 513–520\n",
      "28. Povey D, Ghoshal A, Boulianne G, Burget L, Glembek O, Goel N, Hannemann M, Motlicek P, Qian Y,\n",
      "Schwarz P et al (2011) The Kaldi speech recognition toolkit. In: IEEE Workshop on automatic speech\n",
      "recognition and understanding. IEEE signal processing society\n",
      "29. Qu Z, Haghani P, Weinstein E, Moreno P (2017) Syllable-based acoustic modeling with CTC-SMBR-\n",
      "LSTM. In: IEEE Automatic speech recognition and understanding workshop. IEEE, pp 173–177\n",
      "30. Senior A, Sak H, Shafran I (2015) Context dependent phone models for LSTM RNN acoustic modelling.\n",
      "In: IEEE International conference on acoustics, speech and signal processing. IEEE, pp 4585–4589\n",
      "31. Shi F, Cheng X, Chen X (2012) The summarize of improved HMM Model. In: International conference\n",
      "on computer and information application, pp. 627–630\n",
      "32. Smit P, Virpioja S, Kurimo M (2021) Advances in subword-based HMM-DNN speech recognition across\n",
      "languages. Comput Speech Lang 66:101158\n",
      "41307Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "33. Sundermeyer M, Oparin I, Gauvain JL, Freiberg B, Schl¨ uter R, Ney H (2013) Comparison of feedforward\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 28 ---\n",
      "\n",
      "languages. Comput Speech Lang 66:101158\n",
      "41307Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "33. Sundermeyer M, Oparin I, Gauvain JL, Freiberg B, Schl¨ uter R, Ney H (2013) Comparison of feedforward\n",
      "and recurrent neural network language models. In: IEEE International conference on acoustics, speech\n",
      "and signal processing. IEEE, pp 8430–8434\n",
      "34. TAL Education Group: TAL CS Auto Speech Recognition Data set. Website (2019). https://ai.100tal.\n",
      "com/dataset\n",
      "35. Ueno S, Inaguma H, Mimura M, Kawahara T (2018) Acoustic-to-word attention-based model comple-\n",
      "mented with character-level CTC-based model. In: IEEE International conference on acoustics, speech\n",
      "and signal processing. IEEE, pp 5804–5808\n",
      "36. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017)\n",
      "Attention is all you need. arXiv:1706.03762\n",
      "37. Wang D, Wang X, Lv S (2019) An overview of end-to-end automatic speech recognition. Symmetry\n",
      "11(8):1018\n",
      "38. Wang W, Wang G, Bhatnagar A, Zhou Y, Xiong C, Socher R (2020) An investigation of phone-based\n",
      "subword units for end-to-end speech recognition. arXiv:2004.04290\n",
      "39. Watanabe S, Hori T, Karita S, Hayashi T, Nishitoba J, Unno Y, Soplin NEY, Heymann J, Wiesner M,\n",
      "Chen N et al (2018) ESPnet: End-to-end speech processing toolkit. arXiv:1804.00015\n",
      "40. Woodland PC, Odell JJ, Valtchev V, Young SJ (1994) Large vocabulary continuous speech recognition\n",
      "using HTK. In: IEEE International conference on acoustics, speech and signal processing, vol. 2. IEEE,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 29 ---\n",
      "\n",
      "40. Woodland PC, Odell JJ, Valtchev V, Young SJ (1994) Large vocabulary continuous speech recognition\n",
      "using HTK. In: IEEE International conference on acoustics, speech and signal processing, vol. 2. IEEE,\n",
      "pp II–125\n",
      "41. Wu CH, Shen HP, Yang YT (2014) Chinese-english phone set construction for code-switching ASR\n",
      "using acoustic and DNN-extracted articulatory features. IEEE/ACM Transactions on Audio, Speech, and\n",
      "Language Processing 22(4):858–862\n",
      "42. Yan WQ (2021) Computational methods for deep learning. Springer\n",
      "43. Zenkel T, Sanabria R, Metze F, Waibel A (2017) Subword and crossword units for CTC acoustic models.\n",
      "arXiv:1712.06855\n",
      "44. Zheng Y, Yang X, Dang X (2020) Homophone-based label smoothing in end-to-end automatic speech\n",
      "recognition. arXiv:2004.03437\n",
      "Publisher’snote Springer Nature remains neutral with regard to jurisdictional claims in published maps\n",
      "and institutional affiliations.\n",
      "41308 Multimedia Tools and Applications (2022) 81:41295–41308\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "class TextChunker:\n",
    "\n",
    "    def process(self, pdf_text: str):\n",
    "        \n",
    "        # text_splitter takes strings or a list and returns them in chunks \n",
    "     \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1512,  \n",
    "            chunk_overlap=256,  \n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_text(pdf_text)\n",
    "        if isinstance(chunks, pd.DataFrame):\n",
    "             print(\"The variable is a Pandas DataFrame.\")\n",
    "        else:\n",
    "            print(\"The variable is NOT a Pandas DataFrame.\")\n",
    "        \n",
    "        print(\"Formatted Chunks of Text:\\n\")\n",
    "        for idx, chunk in enumerate(chunks, 1):\n",
    "            print(f\"--- Chunk {idx} ---\\n\")\n",
    "            print(chunk)\n",
    "            print(\"\\n\" + \"-\" * 50 + \"\\n\")  \n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pdf_path = \"/home/pranav/snowflake/chunker_test.pdf\"    \n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = []\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page.page_content)\n",
    "        \n",
    "    pages_text = \"\\n\".join(pages)\n",
    "\n",
    "    chunker = TextChunker()\n",
    "    result = chunker.process(pages_text)\n",
    "    \n",
    "    # for chunk in chunker.process(pages_text):\n",
    "    #     print(chunk)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
