{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.core import Root, CreateMode\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.core.schema import Schema\n",
    "from snowflake.core.stage import Stage\n",
    "from snowflake.core.table import Table, TableColumn, PrimaryKey\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "from snowflake.core.warehouse import Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully\n"
     ]
    }
   ],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "connection_parameters = {\n",
    "        \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "        \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "        \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "       \n",
    "    }\n",
    "\n",
    "\n",
    "try:\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    root = Root(session)\n",
    "    print(\"Connected successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during connection: {e}\")\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created database\n"
     ]
    }
   ],
   "source": [
    "import snowflake.core.database\n",
    "\n",
    "\n",
    "    \n",
    "database = root.databases.create(\n",
    "    Database(\n",
    "        name =\"CORTEX_CONNECT_DB\"),\n",
    "        mode=CreateMode.or_replace\n",
    "    )\n",
    "print(\"created database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = database.schemas.create(\n",
    "    Schema(\n",
    "        name=\"CORTEX_SEARCH_SCHEMA\"),\n",
    "        mode = CreateMode.or_replace,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "Formatted Chunks of Text:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "RESEARCH Open Access\n",
      "Towards end-to-end speech recognition\n",
      "with transfer learning\n",
      "Chu-Xiong Qin 1,2, Dan Qu 1* and Lian-Hai Zhang 1\n",
      "Abstract\n",
      "A transfer learning-based end-to-end speech recognition approach is presented in two levels in our framework.\n",
      "Firstly, a feature extraction approach combining multi lingual deep neural network (DNN) training with matrix\n",
      "factorization algorithm is introduced to extract high-level features. Secondly, the advantage of connectionist\n",
      "temporal classification (CTC) is transferred to the targ et attention-based model through a joint CTC-attention\n",
      "model composed of shallow recurrent neural networks (RNNs) on top of the proposed features. The experimental\n",
      "results show that the proposed transfer learning approach achieved the best performance among all end-to-end\n",
      "methods and could be comparable to the state-of-the-art speech recognition system for TIMIT when further jointly\n",
      "decoded with a RNN language model.\n",
      "Keywords: Speech recognition, End-to-end, Transfer learning\n",
      "1 Introduction\n",
      "A traditional speech recognition system can be divided\n",
      "into several modules such as acoustic models, language\n",
      "models, and decoding. The design of modularization re-\n",
      "lies on many independent assumptions, and even a trad-\n",
      "itional acoustic model is trained in a frame-wise way\n",
      "which depends on Markov assumptions. To eliminate all\n",
      "potential assumptions from an entire speech recognition\n",
      "system and to build a single model optimized in a se-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "itional acoustic model is trained in a frame-wise way\n",
      "which depends on Markov assumptions. To eliminate all\n",
      "potential assumptions from an entire speech recognition\n",
      "system and to build a single model optimized in a se-\n",
      "quence level, the end-to-end method was introduced in\n",
      "the area and has become popular recently [ 1–3]. With\n",
      "the booming development of deep learning methods and\n",
      "under the help of high-performance graphic processing\n",
      "units, end-to-end approaches have been successfully im-\n",
      "plemented in speech recognition. Multiple convolutional\n",
      "and recurrent layers are added in to build an integrated\n",
      "network which acts both as an acoustic model and a lan-\n",
      "guage model, directly mapping speech inputs to tran-\n",
      "scriptions. Specifically, there are two major end-to-end\n",
      "methods for speech recognition, namely connectionist\n",
      "temporal classification (CTC) and attention-based\n",
      "model.\n",
      "In most speech recognition tasks, performance of trad-\n",
      "itional systems still triumph end-to-end approaches [ 4–\n",
      "7]. Many published results have shown that the perform-\n",
      "ance gap between them shrinks with greater amount of\n",
      "training data. For example, success of Baidu ’s Deep\n",
      "Speech [ 4, 8] and Google ’s Listen, Attend, and Spell\n",
      "models [ 9, 10] demonstrated that an end-to-end system\n",
      "benefits in high-resource conditions. A key reason be-\n",
      "hind this conclusion is that current end-to-end models\n",
      "are trained in a data-driven way. All parameters in\n",
      "end-to-end models are updated by computations of gra-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "benefits in high-resource conditions. A key reason be-\n",
      "hind this conclusion is that current end-to-end models\n",
      "are trained in a data-driven way. All parameters in\n",
      "end-to-end models are updated by computations of gra-\n",
      "dients which are easily affected by structures of net-\n",
      "works, so theoretically there is no expert knowledge\n",
      "involved during training. However, end-to-end systems\n",
      "still fail to reach to a state-of-the-art performance even\n",
      "when they are trained with large corpora such as LibriS-\n",
      "peech and Switchboard which have thousands of hours\n",
      "of training data. We assume that end-to-end models suf-\n",
      "fer insufficient training in most cases. In order to\n",
      "neutralize the problem, different variants of networks\n",
      "have been introduced into both CTC and attention-\n",
      "based models. Complex encoders composed of convolu-\n",
      "tional neural networks (CNN) are introduced in order to\n",
      "exploit local correlations in speech signals [ 11–13]. Also\n",
      "joint architectures such as recurrent neural networks\n",
      "(RNNs) combined with conditional random field (CRF)\n",
      "[14], and joint CTC-attention systems [ 15] are proposed.\n",
      "* Correspondence: qudanqudan@sina.com\n",
      "1National Digital Switching System Engineering and Technological R&D\n",
      "Center, Zhengzhou, China\n",
      "Full list of author information is available at the end of the article\n",
      "© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "Center, Zhengzhou, China\n",
      "Full list of author information is available at the end of the article\n",
      "© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\n",
      "International License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\n",
      "reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\n",
      "the Creative Commons license, and indicate if changes were made.\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 \n",
      "https://doi.org/10.1186/s13636-018-0141-9\n",
      "They both take advantages of each sub-model and bring\n",
      "more explicit and strict constraints to the whole model.\n",
      "Although such researches above do improve automatic\n",
      "speech recognition (ASR) performances of end-to-end\n",
      "speech recognition systems, we believe it is hard to find\n",
      "a trade-off strategy between improving complexities of\n",
      "networks and solving low-resource limitations. Though\n",
      "introducing complex computational layers into the\n",
      "model could exploit better correlations in both time and\n",
      "frequency domain, a model with much more parameters\n",
      "would also be harder to train. For end-to-end models,\n",
      "the way of data-driven training without expert know-\n",
      "ledge involved becomes a bottleneck.\n",
      "We propose a transfer learning-based approach that\n",
      "aims to solve the problem with limited speech resource\n",
      "under end-to-end architecture. Previous work has demon-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 5 ---\n",
      "\n",
      "the way of data-driven training without expert know-\n",
      "ledge involved becomes a bottleneck.\n",
      "We propose a transfer learning-based approach that\n",
      "aims to solve the problem with limited speech resource\n",
      "under end-to-end architecture. Previous work has demon-\n",
      "strated that deep learning models across different lan-\n",
      "guages are transferable [ 16, 17], and multi-task learning\n",
      "(MTL) is helpful for end-to-end training [ 14, 15]. In our\n",
      "research, transfer learning is implemented in two levels.\n",
      "Firstly, we extract high-level representations of target\n",
      "speech leveraging a multilingual pre-trained network. We\n",
      "then use nonnegative matrix factorization (NMF) instead\n",
      "of a bottleneck layer to extract high-level speech features,\n",
      "expecting to make the most of nonlinearities of deep\n",
      "neural network (DNN) without breaking their structures\n",
      "during training. Secondly, we build a joint CTC-attention\n",
      "end-to-end model on top of the extracted high-level fea-\n",
      "tures in order to improve robustness through shared\n",
      "training and joint decoding. We use only a shallow\n",
      "bi-directional RNN instead of a complex encoder in [ 18].\n",
      "To exploit as much similarities from multiple data sources\n",
      "as possible, both the end-to-end models and multilingual\n",
      "DNNs are trained in a phone level.\n",
      "Our paper is organized as follows. We describe out\n",
      "method in Section 2 and Section 3. In Section 2, we de-\n",
      "scribe our high-level feature extraction approach with\n",
      "data augmentation. In Section 3, we introduce the joint\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 6 ---\n",
      "\n",
      "DNNs are trained in a phone level.\n",
      "Our paper is organized as follows. We describe out\n",
      "method in Section 2 and Section 3. In Section 2, we de-\n",
      "scribe our high-level feature extraction approach with\n",
      "data augmentation. In Section 3, we introduce the joint\n",
      "CTC-attention model. We introduce our experimental\n",
      "setup and analyze our experimental results in Section 4.\n",
      "At the last section, we provide our conclusions.\n",
      "2 High-level feature extractions with data\n",
      "augmentation\n",
      "Although an ideal end-to-end ASR system aims to build a\n",
      "model mapping from raw inputs to phone/character se-\n",
      "quences, systems using transformed features like fMLLR\n",
      "(feature-space maximum linear logistic regression) fea-\n",
      "tures or introducing complex encoders before RNN tend\n",
      "to perform better compared with those using raw waves\n",
      "or classical features in many tasks [ 6, 11, 13, 19]. We as-\n",
      "sume that transformation in frequency domain could\n",
      "make up for some shortages of limited resource.\n",
      "Classical features such as MFCC or filterbank are only\n",
      "low-level phonetic representations. It requires deep non-\n",
      "linear transformations for such features in end-to-end\n",
      "models. Unlike classical acoustic features, high-level fea-\n",
      "tures are often extracted through DNN and are demon-\n",
      "strated to be high-level semantic representations.\n",
      "2.1 Multilingual training with maxout and dropout\n",
      "In order to further alleviate the problem caused by lim-\n",
      "ited training resource, we use multilingual training in\n",
      "feature extraction as data augmentation. Although\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 7 ---\n",
      "\n",
      "2.1 Multilingual training with maxout and dropout\n",
      "In order to further alleviate the problem caused by lim-\n",
      "ited training resource, we use multilingual training in\n",
      "feature extraction as data augmentation. Although\n",
      "multilingual training is trivial to directly apply on RNNs,\n",
      "it is useful to exploit acoustic similarities from shared\n",
      "layers. We believe that extracting multilingual-based\n",
      "high-level features is an effective way to embed general\n",
      "acoustic knowledge into end-to-end models. Inspired by\n",
      "[15, 18] and our previous work [ 20], we propose two\n",
      "types of feature extractors for end-to-end systems.\n",
      "We first train several target independent DNNs with\n",
      "shared hidden layers using multiple language resources.\n",
      "An example for DNN with multilingual training is\n",
      "shown in Fig. 1. Instead of restricted Boltzmann ma-\n",
      "chine (RBM), we use maxout activation with dropout\n",
      "training to avoid over-fitting problem and to capture\n",
      "better generalities. Three different languages are trained\n",
      "simultaneously, and their senones corresponding to in-\n",
      "puts are considered as supervisions. Two different sche-\n",
      "matic structures of hidden layers are shown in Fig. 2.\n",
      "Figure 2a shows a maxout-dropout hidden structure\n",
      "with a bottleneck layer, and Fig. 2b shows a maxout-\n",
      "dropout hidden structure without a bottleneck layer.\n",
      "For each hidden layer except for the bottleneck layer,\n",
      "activation outputs are described as follows:\n",
      "xl\n",
      "t ¼ ul\n",
      "t /C10 Dt; 1≤l≤L; 1≤t ≤T ð1Þ\n",
      "where ul\n",
      "t is the activation outputs of layer l for tth frame.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 8 ---\n",
      "\n",
      "dropout hidden structure without a bottleneck layer.\n",
      "For each hidden layer except for the bottleneck layer,\n",
      "activation outputs are described as follows:\n",
      "xl\n",
      "t ¼ ul\n",
      "t /C10 Dt; 1≤l≤L; 1≤t ≤T ð1Þ\n",
      "where ul\n",
      "t is the activation outputs of layer l for tth frame.\n",
      "Dt is the same sized vector filled with binary elements\n",
      "each of which represents whether the corresponding\n",
      "Fig. 1 An example for DNN with shared multilingual training\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 2 of 9\n",
      "unit stays non-updated or not. ⊗ stands for the dot\n",
      "product operation.\n",
      "Denote I as actual output number of units in each hid-\n",
      "den layer and suppose the pooling size is 3 with no over-\n",
      "lapping, then there are 3 I units before max pooling.\n",
      "Denote those units as a vector vl\n",
      "t , vl\n",
      "t ¼½ vl\n",
      "tð1Þ; … ; vl\n",
      "tðiÞ; …\n",
      "; vl\n",
      "tð3IÞ/C138ð 1 < i < 3IÞ. The actual maxout outputs are cal-\n",
      "culated as follows:\n",
      "ul\n",
      "t iðÞ¼ max vl\n",
      "t 3i−2ðÞ ; vl\n",
      "t 3i−1ðÞ ; vl\n",
      "t 3iðÞ\n",
      "/C0/C1\n",
      "; 1≤i≤I ð2Þ\n",
      "For the first type, we set bottleneck layers to obtain\n",
      "traditional bottleneck features for end-to-end multilin-\n",
      "gual training. We do not apply dropout for the bottle-\n",
      "neck layer, considering that dropout could do harm to a\n",
      "layer which does not have too many units. Besides,\n",
      "DNNs are supervised by tied triphone units that are gen-\n",
      "erated by Gaussian mixture models (GMM) while inputs\n",
      "are classical low-level acoustic features.\n",
      "It has been demonstrated that lower layers of a DNN\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 9 ---\n",
      "\n",
      "layer which does not have too many units. Besides,\n",
      "DNNs are supervised by tied triphone units that are gen-\n",
      "erated by Gaussian mixture models (GMM) while inputs\n",
      "are classical low-level acoustic features.\n",
      "It has been demonstrated that lower layers of a DNN\n",
      "are transferable to new classification tasks. For the\n",
      "multilingual pre-trained network that includes a bottle-\n",
      "neck layer, we transfer all layers below the bottleneck\n",
      "layer to the target bottleneck DNN. Then, we stack a\n",
      "hidden layer and a softmax layer on top of them to build\n",
      "the target DNN. For the pre-trained network without\n",
      "setting a bottleneck layer, we propose a very different ap-\n",
      "proach. This is motivated by a hypothesis that setting a\n",
      "bottleneck layer degrades the classification accuracy of a\n",
      "DNN, which is also harmful to the bottleneck features\n",
      "themselves. Therefore, we would like to extract\n",
      "low-dimensional features from DNN without bottleneck\n",
      "layers. We first transfer all parameters below the last\n",
      "hidden layer from Fig. 2b and add a new softmax layer\n",
      "with random parameters to initialize the target DNN,\n",
      "and then we fine-tune the whole target DNN. Such\n",
      "adapt training without breaking the structure during\n",
      "training enables us keep maximum nonlinearity for later\n",
      "processing.\n",
      "2.2 High-level feature extractions using NMF\n",
      "We need to do dimensionality reduction to extract fea-\n",
      "tures since a high-dimensional vector output from a hid-\n",
      "den layer contains many redundant values. Although\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 10 ---\n",
      "\n",
      "processing.\n",
      "2.2 High-level feature extractions using NMF\n",
      "We need to do dimensionality reduction to extract fea-\n",
      "tures since a high-dimensional vector output from a hid-\n",
      "den layer contains many redundant values. Although\n",
      "this approach is in a two-step fashion, it would be better\n",
      "if the dimension-reduction were associated closely with\n",
      "the DNN so that the features could benefit most from\n",
      "supervision of phone/state indirectly. Since it is obvious\n",
      "that weight matrices in DNN determine how vectors of\n",
      "hidden representations are formed, we apply matrix\n",
      "factorization algorithms on weight matrix of DNN in-\n",
      "stead of directly implementing naive dimensionality re-\n",
      "duction algorithms on hidden outputs.\n",
      "In this paper, we adopt convex nonnegative matrix\n",
      "factorization (CNMF) to extract high-level features.\n",
      "NMF has advantages over singularly valuable decompos-\n",
      "ition (SVD) and principal components analysis (PCA) in\n",
      "this problem. SVD and PCA are mathematically equivalent\n",
      "when dealing with the dimens ionality reduction problem\n",
      "[21]. When using SVD to compact networks [ 22], we only\n",
      "select a certain amount of singular values. Both\n",
      "left-singular matrix and right-singular matrix contain much\n",
      "nonlinearities of the target weight matrix. Therefore\n",
      "through ignoring some component from matrices to form\n",
      "a linear project layer would cause certain losses. Compared\n",
      "with SVD, almost all NMF algorithms require at least one\n",
      "matrix to be nonnegative. Therefore, the target matrix\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 11 ---\n",
      "\n",
      "through ignoring some component from matrices to form\n",
      "a linear project layer would cause certain losses. Compared\n",
      "with SVD, almost all NMF algorithms require at least one\n",
      "matrix to be nonnegative. Therefore, the target matrix\n",
      "could be defined as weighted sum of columns in the base\n",
      "m a t r i x .N o t et h a tt h i si sav e ry important constraint be-\n",
      "cause this makes the coefficient matrix much less import-\n",
      "ant when we only want to keep the basic part of the\n",
      "original matrix. This explains why NMF is more interpret-\n",
      "able than SVD and PCA when dealing with our problem.\n",
      "NMF is demonstrated to discover base features embedded\n",
      "in matrices [ 23] which we believe would be useful for\n",
      "limited-resource condition. Additionally, we do not use ori-\n",
      "ginal NMF because it only deals with nonnegative values\n",
      "which are not the case for weight matrices of DNN.\n",
      "As a variant of NMF, CNMF not only has no hard\n",
      "bound for values, but also restrict the base matrix to be\n",
      "Fig. 2 Two structures of DNN hidden layers. a Maxout-dropout hidden structure with a bottleneck layer. b Maxout-dropout hidden structure\n",
      "without bottleneck layer\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 3 of 9\n",
      "convex combinations of columns in the target matrix.\n",
      "Assuming the target weight matrix X has a size of N ×\n",
      "M, it is factorized into a base matrix F (with a size of\n",
      "N × R) and a coefficient matrix G (with a size of R × M).\n",
      "We first use K-means to initialize CNMF, as described in\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 12 ---\n",
      "\n",
      "Assuming the target weight matrix X has a size of N ×\n",
      "M, it is factorized into a base matrix F (with a size of\n",
      "N × R) and a coefficient matrix G (with a size of R × M).\n",
      "We first use K-means to initialize CNMF, as described in\n",
      "[23], to obtain cluster indicators H =( h1, … , hk). h1, … ,\n",
      "hk are vectors containing binary values. Then, we\n",
      "initialize G with\n",
      "G 0ðÞ ¼ H þ 0:2E; ð3Þ\n",
      "where E is an identity matrix. F is initialized as cluster\n",
      "centroids:\n",
      "F ¼ XHD−1\n",
      "n ; ð4Þ\n",
      "where Dn = diag(n1, … , nk) and n1, … , nk are numbers of\n",
      "classes. CNMF defines F to be linear combinations of\n",
      "columns of X, which is F = XW. Therefore we get Wð0Þ\n",
      "¼ HD−1\n",
      "n according to this constraint and (4). Then G\n",
      "and W are updated as follows until convergence:\n",
      "Gik\n",
      "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n",
      "XT X\n",
      "/C0/C1 þ\n",
      "W\n",
      "hi\n",
      "ik\n",
      "þ GWT XT X\n",
      "/C0/C1 −\n",
      "W\n",
      "/C2/C3\n",
      "ik\n",
      "XT X\n",
      "/C0/C1 −\n",
      "W\n",
      "/C2/C3\n",
      "ik þ GWT XT X\n",
      "/C0/C1 þ\n",
      "W\n",
      "hi\n",
      "ik\n",
      "vuuut →Gik\n",
      "Wik\n",
      "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n",
      "XT X\n",
      "/C0/C1 þ\n",
      "G\n",
      "hi\n",
      "ik\n",
      "þ XT X\n",
      "/C0/C1 −\n",
      "WGT G\n",
      "/C2/C3\n",
      "ik\n",
      "XT X\n",
      "/C0/C1 −\n",
      "G\n",
      "/C2/C3\n",
      "ik þ XT X\n",
      "/C0/C1 þ\n",
      "WGT G\n",
      "hi\n",
      "ik\n",
      "vuuut →Wik\n",
      "8\n",
      ">>>>>>>><\n",
      ">>>>>>>>:\n",
      "ð5Þ\n",
      "where ( ⋅)− and ( ⋅)+ denotes generalized {1}-inverse and\n",
      "Moore–Penrose pseudoinverse respectively. After train-\n",
      "ing, the base matrix F and the coefficient matrix G are\n",
      "obtained.\n",
      "Figure 3 shows high-level feature extraction through\n",
      "applying CNMF on a specific hidden layer. Weights\n",
      "matrix of a certain hidden layer is factorized into two\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 13 ---\n",
      "\n",
      "ing, the base matrix F and the coefficient matrix G are\n",
      "obtained.\n",
      "Figure 3 shows high-level feature extraction through\n",
      "applying CNMF on a specific hidden layer. Weights\n",
      "matrix of a certain hidden layer is factorized into two\n",
      "matrices following the above process. We abandon G\n",
      "and set F to be the weight matrix of the new feature ex-\n",
      "traction layer. The new layer is functionally similar to\n",
      "bottleneck layer, except that we calculate the features\n",
      "without the bias variable:\n",
      "f ¼ FT u ¼ WT XT u ð6Þ\n",
      "We can see from (6) that the features are transformed\n",
      "on original hidden outputs XTu via WT. Note that W is\n",
      "trained with original weight matrix X involved and act\n",
      "as a matrix for dimensionality reduction; therefore, we\n",
      "believe that our high-level features could capture\n",
      "multi-lingual acoustic similarities and at the mean time\n",
      "alleviate sparse problem caused by limited resource.\n",
      "3 Joint CTC-attention\n",
      "In this section, we will describe the end-to-end model of our\n",
      "transfer learning approach on top of our high-level features.\n",
      "Following the structure in [ 18], our end-to-end architecture\n",
      "is a joint CTC-attention model which consists of a shared\n",
      "encoder and a joint decoder. Our design is to transfer mono-\n",
      "tonic constraint from CTC to the target attention-based\n",
      "model to improve accuracy through joint modeling.\n",
      "3.1 Joint training with shared encoder\n",
      "The network for feature extraction could also be consid-\n",
      "ered as intermediate supervision for our end-to-end\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 14 ---\n",
      "\n",
      "tonic constraint from CTC to the target attention-based\n",
      "model to improve accuracy through joint modeling.\n",
      "3.1 Joint training with shared encoder\n",
      "The network for feature extraction could also be consid-\n",
      "ered as intermediate supervision for our end-to-end\n",
      "models. Therefore, we believe that our features already\n",
      "contain high-level speech information so that it is not\n",
      "necessary to build a deep VGG encoder which maps\n",
      "from raw inputs to phones. In this paper, we only build\n",
      "a shallow RNN with bi-directional long short-term\n",
      "memory (BLSTM) cells instead.\n",
      "In this section, we stack joint CTC-attention model on\n",
      "top of our high-level features to build an end-to-end\n",
      "model which is shown in Fig. 4.\n",
      "In shared encoder, the posteriors of a symbol πt at\n",
      "time t are computed over all high-level inputs X:\n",
      "p πtjXðÞ ¼ Softmax BLSTM XðÞðÞ ð 7Þ\n",
      "Then the probability distribution p(S| X) over all pos-\n",
      "sible phone sequence S' is modeled under conditional in-\n",
      "dependent assumptions:\n",
      "Fig. 3 High-level features extraction using CNMF\n",
      "Fig. 4 Joint CTC-attention model with a shallow encoder on\n",
      "high-level features\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 4 of 9\n",
      "pCTC SjXðÞ ¼\n",
      "X\n",
      "π∈Φ S\n",
      "0\n",
      "ðÞ\n",
      "p πjXðÞ ≈\n",
      "X\n",
      "π∈Φ S\n",
      "0\n",
      "ðÞ\n",
      "YT\n",
      "t¼1\n",
      "p πtjXðÞ\n",
      "ð8Þ\n",
      "For the attention-based part, the model is composed\n",
      "of three components. The encoder is the shared BLSTM.\n",
      "The attention layer is location based. Denote αk, t as the\n",
      "attention weights connecting kth encoder outputs and\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 15 ---\n",
      "\n",
      "X\n",
      "π∈Φ S\n",
      "0\n",
      "ðÞ\n",
      "YT\n",
      "t¼1\n",
      "p πtjXðÞ\n",
      "ð8Þ\n",
      "For the attention-based part, the model is composed\n",
      "of three components. The encoder is the shared BLSTM.\n",
      "The attention layer is location based. Denote αk, t as the\n",
      "attention weights connecting kth encoder outputs and\n",
      "tth decoder inputs. αk, t are calculated using the previous\n",
      "weights αk − 1, t, the hidden outputs for decoder qk − 1,\n",
      "and the encoder outputs ht:\n",
      "fn ¼ F /C3 αn−1 ð9Þ\n",
      "ek;l ¼ wT tanh VSsn þ VH hn þ VF fk;l þ b\n",
      "/C0/C1\n",
      "ð10Þ\n",
      "αk;t ¼ exp αek;t\n",
      "/C0/C1\n",
      "PT\n",
      "t¼1 exp αek;t\n",
      "/C0/C1 ð11Þ\n",
      "rk ¼\n",
      "XT\n",
      "t¼1αk;tht ð12Þ\n",
      "ps k jsk ; … ; sk−1; XðÞ ¼ Decoder rk ; qk−1; htðÞ ð 13Þ\n",
      "F is a convolution filter and αn is a T-dimensional at-\n",
      "tention weight vector. w, VS, VH, and VF are trainable\n",
      "weight matrices of multilayer perceptron (MLP). rk is a\n",
      "context vector that integrates all encoder outputs using\n",
      "attention weights.\n",
      "The posteriors p(S| X) of attention-based model are\n",
      "estimated without any conditional assumptions:\n",
      "patt SjXðÞ ≈\n",
      "Y\n",
      "k\n",
      "ps k js1; … ; sk−1; XðÞ ð 14Þ\n",
      "The loss functions of CTC and attention-based models\n",
      "to be optimized are defined as:\n",
      "ℒCTC ¼ − lnpCTC SjXðÞ\n",
      "ℒatt ¼ − lnpatt SjXðÞ\n",
      "/C26\n",
      "ð15Þ\n",
      "The total loss function to be optimized is calculated as\n",
      "a combination of logarithmic linear function of CTC\n",
      "and attention:\n",
      "ℒt ¼ λℒCTC þ 1−λðÞ ℒattention\n",
      "0\n",
      "λ∈ 0; 1½/C138 ð 16Þ\n",
      "where λ is the linear weight of CTC loss.\n",
      "3.2 Joint decoder\n",
      "In the previous subsection, we incorporate CTC object-\n",
      "ive into joint training to enhance the attention-based\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 16 ---\n",
      "\n",
      "and attention:\n",
      "ℒt ¼ λℒCTC þ 1−λðÞ ℒattention\n",
      "0\n",
      "λ∈ 0; 1½/C138 ð 16Þ\n",
      "where λ is the linear weight of CTC loss.\n",
      "3.2 Joint decoder\n",
      "In the previous subsection, we incorporate CTC object-\n",
      "ive into joint training to enhance the attention-based\n",
      "model. In this subsection, we will describe details of the\n",
      "joint decoder of our model.\n",
      "For the attention decoder, it computes the score of hy-\n",
      "pothesis in the beam search recursively:\n",
      "αatt gl\n",
      "/C0/C1\n",
      "¼ αatt gl−1\n",
      "/C0/C1\n",
      "þ logps jgl−1; X\n",
      "/C0/C1\n",
      "ð17Þ\n",
      "where gl is a hypothesis with length l, and s is the last\n",
      "character of gl.\n",
      "It is known that the attention-based model decodes\n",
      "phone/character synchronously while CTC does it in a\n",
      "frame-wise way. We use CTC prefix probability and ob-\n",
      "tain the CTC score as:\n",
      "αCTC gl\n",
      "/C0/C1\n",
      "¼ logpg l; … jX\n",
      "/C0/C1\n",
      "ð18Þ\n",
      "We combine CTC score and attention score together\n",
      "using one-pass decoding following the method in [ 24],\n",
      "then αCTC(gl) can be combined with αatt(gl) using λ.T h e\n",
      "joint decoder gives the most probable phone sequence ^S\n",
      "following:\n",
      "^S ¼ arg max\n",
      "S\n",
      "λαCTC gl\n",
      "/C0/C1\n",
      "þ 1−λðÞ αatt gl\n",
      "/C0/C1/C8/C9\n",
      "ð19Þ\n",
      "After CTC-attention multi-task learning, the part of\n",
      "attention-based model is used as the target model for\n",
      "recognition, and the part of CTC model helps the target\n",
      "attention-based model in the decoding stage.\n",
      "Note that although joint CTC-attention models are\n",
      "already proposed, it has not been proved to be effective\n",
      "either with transformed features or under limited condi-\n",
      "tions. Also, different from the deep structures in the ori-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 17 ---\n",
      "\n",
      "Note that although joint CTC-attention models are\n",
      "already proposed, it has not been proved to be effective\n",
      "either with transformed features or under limited condi-\n",
      "tions. Also, different from the deep structures in the ori-\n",
      "ginal design, the encoders of our joint models are only\n",
      "composed of shallow RNNs.\n",
      "4 Experiments and results\n",
      "4.1 Data and experimental setup\n",
      "We evaluate and train our end-to-end models on TIMIT\n",
      "dataset, and we use several language resources of Vox-\n",
      "forge dataset to do multilingual training. The augment-\n",
      "ing resources we use are listed in Table 1.\n",
      "All of our end-to-end models are trained with 48 pho-\n",
      "nemes, and there predictions are converted to 39 pho-\n",
      "nemes for scoring. All SA sentences are removed. We\n",
      "followed the common setup of TIMIT. Four hundred\n",
      "sixty-two speakers are selected for train set, 50 speakers\n",
      "are selected for dev set, and 24 speakers are selected for\n",
      "core test set. Durations of them are listed in Table 2.\n",
      "We first train a CTC and an attention-based model\n",
      "separately. Forty mel-scale filterbank coefficients and\n",
      "Table 1 Information of Voxforge multilingual resources\n",
      "No. Language Duration (h)\n",
      "1 Italian 13.720\n",
      "2 German 43.173\n",
      "3 French 25.165\n",
      "4 Spanish 7.785\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 5 of 9\n",
      "their delta and delta-delta features are concatenated as\n",
      "their input features.\n",
      "The baseline CTC model is a five-layer BLSTM with\n",
      "256 cells in each layer and direction. Dropout rate of 0.2\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 18 ---\n",
      "\n",
      "their delta and delta-delta features are concatenated as\n",
      "their input features.\n",
      "The baseline CTC model is a five-layer BLSTM with\n",
      "256 cells in each layer and direction. Dropout rate of 0.2\n",
      "and 0.5 are applied on inputs and BLSTM separately.\n",
      "For the attention-based model, the encoder is a\n",
      "four-layer BLSTM with 256 cells in each layer and direc-\n",
      "tion. The attention layer is location-based and has 160\n",
      "cells. The decoder is a one-layer LSTM with 256 cells.\n",
      "Dropout rates for the inputs, the encoder, the attention\n",
      "layer, and the decoder are 0.2, 0.5, 0.1, and 0.1 respect-\n",
      "ively. The optimizer is Adam [ 25] for both models.\n",
      "For all end-to-end models, gradient clipping [ 26]i s\n",
      "used for all end-to-end training, and the gradient norm\n",
      "threshold to clip is set to be 5.0. Also, the batch sizes\n",
      "during training are all set to be 32.\n",
      "4.2 Evaluation measurement\n",
      "Phone error rate (PER) is adopted to measure the per-\n",
      "formance of speech recognition systems. This score is\n",
      "calculated with the following equation:\n",
      "PER ¼ nIns þ nDel þ nSub\n",
      "N /C2 100% ð20Þ\n",
      "where nIns, nDel,a n d nSub are number of insert errors,\n",
      "delete errors, and substitute errors, respectively. N is the\n",
      "total number of phones in the ground truth labels.\n",
      "4.3 Transfer learning based experiments\n",
      "We then evaluate on our proposed methods. Firstly, a\n",
      "joint CTC-attention model is trained with the same fil-\n",
      "terbank features. We compare two structures according\n",
      "to our experience. For CTC4 + att4, the CTC part is a\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 19 ---\n",
      "\n",
      "We then evaluate on our proposed methods. Firstly, a\n",
      "joint CTC-attention model is trained with the same fil-\n",
      "terbank features. We compare two structures according\n",
      "to our experience. For CTC4 + att4, the CTC part is a\n",
      "four-layer BLSTM with 256 cells and the attention-\n",
      "model has a four-layer BLSTM encoder with 256 cells in\n",
      "each layer and direction, an attention layer with 128\n",
      "cells, and a one-layer LSTM decoder with 256 cells. The\n",
      "CTC5 + att4 only has more layers of BLSTM for the\n",
      "CTC part, and the rests are the same. For the rest of the\n",
      "joint models in our experiments, they are named by the\n",
      "same manner. For all joint end-to-end models, we set\n",
      "most of the configurations to be the same. The\n",
      "optimizer is AdaDelta [ 27] function with a learning rate\n",
      "of 10 –3. Dropout rate of 0.2 is applied to all BLSTM\n",
      "layers. The MTL weight and the decoding weight for\n",
      "CTC are both 0.3. The width for beam research in de-\n",
      "coding stage is 20. The model which has the best\n",
      "accuracy evaluated by dev set is chosen to be the final\n",
      "model after 30 epochs of training.\n",
      "We then experiment with our multilingual methods.\n",
      "We note that TIMIT is evaluated by phones instead of\n",
      "characters. Therefore, each pronunciation dictionary of\n",
      "Voxforge resources is generated using G2P toolkit [ 28]\n",
      "respectively under the help of CMU dictionary [ 29]. This\n",
      "allows DNNs to capture enough acoustic similarities.\n",
      "To train multilingual DNNs, we first build their own\n",
      "GMMs. Each GMM is trained using linear discriminative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 20 ---\n",
      "\n",
      "respectively under the help of CMU dictionary [ 29]. This\n",
      "allows DNNs to capture enough acoustic similarities.\n",
      "To train multilingual DNNs, we first build their own\n",
      "GMMs. Each GMM is trained using linear discriminative\n",
      "analysis (LDA), maximum linear logistic transformation\n",
      "(MLLT), and speaker adaptive training (SAT) with 13\n",
      "MFCC features. Then multilingual DNNs are trained\n",
      "with 40 filterbank features of four Voxforge languages\n",
      "simultaneously and are all supervised by alignments of\n",
      "senones generated by each GMM. Numbers of senones\n",
      "for four languages are listed in Table 3.\n",
      "We train our end-to-end models with two types of\n",
      "high-level features as described in Section 2, 4langA-\n",
      "daptBN and 4langAdaptCNMF. For the 4langAdaptBN\n",
      "system, a multilingual bottleneck DNN is trained by four\n",
      "language resources in Voxforge dataset. Each hidden\n",
      "layer has 1026 units (342 units after max pooling) and\n",
      "the bottleneck layer has 120 units (40 units after max\n",
      "pooling). For the 4langAdaptCNMF system, a multilin-\n",
      "gual DNN without bottleneck layers is trained with\n",
      "1026 units (342 units after max pooling) in each hidden\n",
      "layer.\n",
      "All DNNs are trained with a dropout rate of 0.2 for\n",
      "other hidden layers. All maxout groups have a pooling\n",
      "size of 3. The Initial learning rate is kept 0.2 for the first\n",
      "8 epochs and after which is decayed by half when valid-\n",
      "ation error does not decline. The training stops when\n",
      "the validation error finally increases.\n",
      "For the next step, all parameters below the last shared\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 21 ---\n",
      "\n",
      "8 epochs and after which is decayed by half when valid-\n",
      "ation error does not decline. The training stops when\n",
      "the validation error finally increases.\n",
      "For the next step, all parameters below the last shared\n",
      "hidden layer of the multilingual DNN are transferred to\n",
      "a new DNN which is then also re-trained by TIMIT (For\n",
      "the bottleneck multilingual DNN, all parameters below\n",
      "shared bottleneck layer are transferred to a new bottle-\n",
      "neck DNN which is then also re-trained by TIMIT). For\n",
      "the bottleneck features, they are extracted from the\n",
      "re-trained bottleneck layer by feed forward inputs. For\n",
      "the CNMF-based features, we follow exactly the same\n",
      "settings in [ 20] in our experiments since the approach is\n",
      "sensible to layers and dimensions accorded from the\n",
      "Table 2 Durations of TIMIT dataset\n",
      "Set Duration (h)\n",
      "train 3.145\n",
      "dev 0.342\n",
      "test 0.162\n",
      "Table 3 Numbers of senones for four languages\n",
      "No. Language Number of\n",
      "senones in GMM\n",
      "1 Italian 1528\n",
      "2 German 1544\n",
      "3 French 1400\n",
      "4 Spanish 1568\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 6 of 9\n",
      "experience of our prior work. CNMF is applied on the\n",
      "weight matrix from the second last hidden layer, and the\n",
      "dimension for factorization is 40 with 5000 iterations of\n",
      "training. The CNMF-based features are extracted follow-\n",
      "ing the steps described in Section 2.\n",
      "Our high-level features are then sent into joint\n",
      "CTC-attention models for end-to-end training. Unlike a\n",
      "baseline setup, only shallow RNN networks are built for\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 22 ---\n",
      "\n",
      "training. The CNMF-based features are extracted follow-\n",
      "ing the steps described in Section 2.\n",
      "Our high-level features are then sent into joint\n",
      "CTC-attention models for end-to-end training. Unlike a\n",
      "baseline setup, only shallow RNN networks are built for\n",
      "the models. Here we experiment on two structures,\n",
      "namely CTC2 + att2 and CTC3 + att2. We also adjust\n",
      "configurations for these shallow joint models in order to\n",
      "achieve the best performances. Besides different types of\n",
      "input features and number of layers, BLSTM and LSTM\n",
      "are both set to have 320 cells for each layer and direc-\n",
      "tion, and the location-based attention layer has 160 cells.\n",
      "4.4 Results and discussions\n",
      "Table 4 shows PERs of all referenced methods and our\n",
      "methods on TIMIT core test set. Below the first line, the\n",
      "first block and the second block shows performances of\n",
      "referenced traditional systems and end-to-end systems\n",
      "respectively. The performances of our results are sum-\n",
      "marized in the last block. We can see that performances\n",
      "of our baseline systems (filterbank + CTC, filterbank +\n",
      "att) cannot compare with referenced methods which use\n",
      "transformed features such as fMLLRs and complex net-\n",
      "works. This is much due to the fact that TIMIT provides\n",
      "a limited resource condition in which purely data-driven\n",
      "training could not satisfied.\n",
      "For baseline systems, the joint models (filterbank +\n",
      "CTC4 + att4, filterbank + CTC5 + att4) perform better\n",
      "than baseline systems. This demonstrates that joint\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 23 ---\n",
      "\n",
      "a limited resource condition in which purely data-driven\n",
      "training could not satisfied.\n",
      "For baseline systems, the joint models (filterbank +\n",
      "CTC4 + att4, filterbank + CTC5 + att4) perform better\n",
      "than baseline systems. This demonstrates that joint\n",
      "training and decoder benefits for end-to-end models.\n",
      "We can also conclude from results of filterbank-based\n",
      "systems (filterbank + CTC4 + att4, filterbank + CTC5 +\n",
      "att4), bottleneck-based systems (4langAdaptBN + CTC2\n",
      "+ att2, 4langAdaptBN + CTC3 + att2), and NMF-based\n",
      "systems (4langAdaptCNMF + CTC2 + att2, 4langA-\n",
      "daptCNMF + CTC3 + att2) that a joint model perform\n",
      "better when CTC has one more BLSTM layer than en-\n",
      "coder of attention-based model.\n",
      "However, performances of filterbank-based systems are\n",
      "still inferior to listed methods due to the lack of trans-\n",
      "formations and regularizations under limited resource\n",
      "condition. The fact that the filterbank-based joint\n",
      "models even perform worse than the CTC baseline sys-\n",
      "tem demonstrates that joint modeling is not enough for\n",
      "solving problem with limited resource.\n",
      "When multilingual pre-trained bottleneck features are\n",
      "brought in, our systems (4langAdaptBN + CTC2 + att2,\n",
      "4langAdaptBN + CTC3 + att2) achieve 18.63% and\n",
      "18.28% on PERs which are comparable to some refer-\n",
      "enced results but are still no better than best end-to-end\n",
      "results. We believe that the disadvantage of setting a\n",
      "bottleneck layer which is analyzed in Section 2 is re-\n",
      "sponsible for this.\n",
      "When using CNMF to extract features, our best sys-\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 24 ---\n",
      "\n",
      "enced results but are still no better than best end-to-end\n",
      "results. We believe that the disadvantage of setting a\n",
      "bottleneck layer which is analyzed in Section 2 is re-\n",
      "sponsible for this.\n",
      "When using CNMF to extract features, our best sys-\n",
      "tem (4langAdaptCNMF + CTC3 + att2) obtains a PER of\n",
      "16.96%. This is superior to all published end-to-end\n",
      "methods in Table 4. Also the fact that NMF approach\n",
      "performs better than bottleneck approach supports our\n",
      "hypothesis (in Section 2.1) that the existence of bottle-\n",
      "neck layers degrades the classification accuracies and\n",
      "could not fully exploit deep transformation of the\n",
      "pre-trained network. These results also strongly support\n",
      "the effectiveness of our CNMF-based approach in\n",
      "end-to-end models.\n",
      "Although our transfer learning approach requires extra\n",
      "training procedures to extract features, it performs bet-\n",
      "ter with less RNN layers for the end-to-end part.\n",
      "We also list PERs of some representative traditional\n",
      "methods and notice that published end-to-end models\n",
      "could not beat traditional speech recognition systems.\n",
      "To further compare with the best traditional system\n",
      "which has a language model (LM), we add a small\n",
      "RNN-LM to decode jointly following the method in\n",
      "[18]. The RNN is a two-layer LSTM with 256 cells in\n",
      "each layer and is trained using all transcriptions from\n",
      "the TIMIT train set with a batch size of 32. The LM\n",
      "weight for decoding is 0.2. The result in the last line of\n",
      "Table 4 shows that PER further decreased to 16.59%,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 25 ---\n",
      "\n",
      "each layer and is trained using all transcriptions from\n",
      "the TIMIT train set with a batch size of 32. The LM\n",
      "weight for decoding is 0.2. The result in the last line of\n",
      "Table 4 shows that PER further decreased to 16.59%,\n",
      "which is not only the best among all listed end-to-end\n",
      "results but also is comparable to the state-of-the-art\n",
      "Table 4 PERs of different speech recognition systems on TIMIT\n",
      "core set\n",
      "System PER (%)\n",
      "Kaldi’s DNN-HMM 18.5\n",
      "Hierarchical maxout CNN [ 30] 16.5\n",
      "Raw speech + WaveNet [ 19] 18.8\n",
      "filterbank + CTC + weight noise [ 31] 18.4\n",
      "hierarchical CNNs with CTC [ 32] 18.2\n",
      "Raw speech + complex ConvNets [ 33] 18.0\n",
      "RNN transducer initialized with CTC + weight noise [ 31] 17.7\n",
      "fMLLR + Attention + weight noise [ 6] 17.6\n",
      "fMLLR + RNN + CRF [ 14] 17.3\n",
      "filterbank + CTC5 18.66\n",
      "filterbank + att4 20.49\n",
      "filterbank + CTC4 + att4 19.85\n",
      "filterbank + CTC5 + att4 19.14\n",
      "4langAdaptBN + CTC2 + att2 18.63\n",
      "4langAdaptBN + CTC3 + att2 18.28\n",
      "4langAdaptCNMF + CTC2 + att2 17.70\n",
      "4langAdaptCNMF + CTC3 + att2 16.96\n",
      "4langAdaptCNMF + CTC3 + att2 + RNN-LM 16.59\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 7 of 9\n",
      "performance in TIMIT. Note that our end-to-end\n",
      "models are trained without any regularization except for\n",
      "dropout on BLSTM layers. We believe that our models\n",
      "would benefit more if they were trained with designed\n",
      "regularizations and bigger RNN-LM.\n",
      "5 Conclusions\n",
      "A novel transfer learning-based approach is proposed\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 26 ---\n",
      "\n",
      "dropout on BLSTM layers. We believe that our models\n",
      "would benefit more if they were trained with designed\n",
      "regularizations and bigger RNN-LM.\n",
      "5 Conclusions\n",
      "A novel transfer learning-based approach is proposed\n",
      "for end-to-end speech recognition. For the first stage,\n",
      "NMF together with multilingual training are used to ex-\n",
      "tract high-level features. For the second stage, joint\n",
      "CTC-attention models are trained on top of the\n",
      "high-level features. Transfer learning is applied through\n",
      "multilingual training and multi-task learning in two\n",
      "levels. Experiments on TIMIT show that our model per-\n",
      "forms the best among all end-to-end models and\n",
      "achieves an extremely close performance compared with\n",
      "the state-of-the-art speech recognition system.\n",
      "Although our transfer learning approach improves per-\n",
      "formances of end-to-end speech recognition models in\n",
      "TIMIT, it needs to be tested whether this approach also\n",
      "works for relatively high-resource (more than thousands\n",
      "of hour ’s data) end-to-end training. What is more is this\n",
      "approach remains a two-stage training fashion which is\n",
      "not a standard end-to-end way. Therefore optimizing fea-\n",
      "ture extraction and RNN training with only one objective\n",
      "function is another job to do. This would require proper\n",
      "tasks separation and intermediate supervisions.\n",
      "Funding\n",
      "This work was supported in part by the National Natural Science Foundation\n",
      "of China (No. 61673395, and No. 61403415), Natural Science Foundation of\n",
      "Henan Province (No. 162300410331).\n",
      "Authors’ contributions\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 27 ---\n",
      "\n",
      "Funding\n",
      "This work was supported in part by the National Natural Science Foundation\n",
      "of China (No. 61673395, and No. 61403415), Natural Science Foundation of\n",
      "Henan Province (No. 162300410331).\n",
      "Authors’ contributions\n",
      "C-XQ and DQ conceived and designed the study. C-XQ performed the exper-\n",
      "iments. C-XQ and DQ wrote the paper. C-XQ, DQ, and L-HZ reviewed and\n",
      "edited the manuscript. All authors read and approved the manuscript.\n",
      "Authors’ information\n",
      "Chu-Xiong Qin was born in Shijiazhuang, China, in 1991. He received the B.S.\n",
      "and M.S. degrees in information and communication from the National Digital\n",
      "Switching System Engineering and Technological R&D Center, Zhengzhou,\n",
      "China, in 2013 and 2016, respectively. He is currently working towards the Ph.D.\n",
      "degree on speech recognition at the National Digital Switching System\n",
      "Engineering and Technological R&D Center. His research interests are in speech\n",
      "signal processing, continuous speech recognition, and machine learning.\n",
      "Dan Qu received the M.S. degree in communication and information system\n",
      "from Xi ’an Information Science and Technology Institute, Xi ’an, China, in\n",
      "2000 and the Ph.D. degree in information and communication engineering\n",
      "from the National Digital Switching System Engineering and Technological R&D\n",
      "Center, Zhengzhou, China, in 2005. She is an Associate Professor at the National\n",
      "Digital Switching System Engineering and Technological R&D Center. Her\n",
      "research interests are in speech signal processing and pattern recognition.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 28 ---\n",
      "\n",
      "Center, Zhengzhou, China, in 2005. She is an Associate Professor at the National\n",
      "Digital Switching System Engineering and Technological R&D Center. Her\n",
      "research interests are in speech signal processing and pattern recognition.\n",
      "Lian-Hai Zhang received the M.S. degree in information and communication\n",
      "engineering from the National Digital Switching System Engineering and\n",
      "Technological R&D Center, Zhengzhou, China, in 2000. He is an Associate\n",
      "Professor at the National Digital Switching System Engineering and\n",
      "Technological R&D Center. His research interests are in speech signal\n",
      "processing.\n",
      "Competing interests\n",
      "The authors declare that they have no competing interests.\n",
      "Publisher’sN o t e\n",
      "Springer Nature remains neutral with regard to jurisdictional claims in\n",
      "published maps and institutional affiliations.\n",
      "Author details\n",
      "1National Digital Switching System Engineering and Technological R&D\n",
      "Center, Zhengzhou, China. 2The State Key Laboratory of Integrated Services\n",
      "Networks, Xidian University, Xi ’an, China.\n",
      "Received: 29 July 2018 Accepted: 28 September 2018\n",
      "References\n",
      "1. Graves, A., & Gomez, F. (2006). Connectionist temporal classification:\n",
      "Labelling unsegmented sequence data with recurrent neural networks. In\n",
      "International Conference on Machine learning (ICML) (pp. 369 – 376).\n",
      "2. Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end\n",
      "continuous speech recognition using attention-based recurrent NN: First\n",
      "results. arXiv preprint arXiv, v1412 , 1602.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 29 ---\n",
      "\n",
      "2. Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end\n",
      "continuous speech recognition using attention-based recurrent NN: First\n",
      "results. arXiv preprint arXiv, v1412 , 1602.\n",
      "3. Graves, A., & Jaitly, N. (2014). Towards end-to-end speech recognition with\n",
      "recurrent neural networks. In International Conference on Machine Learning\n",
      "(ICML) (pp. 1764 – 1772).\n",
      "4. D. Amodei, R. Anubhai, E. Battenberg, et al , “Deep speech 2: End-to-end\n",
      "speech recognition in English and mandarin, ” Computer Science, 2015.\n",
      "5. Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). Librispeech: An\n",
      "ASR corpus based on public domain audio books. In IEEE International\n",
      "Conference on Acoustics, Speech and Signal Processing (ICASSP) .\n",
      "6. Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., & Bengio, Y. (2015).\n",
      "Attention-based models for speech recognition. In Advances in Neural\n",
      "Information Processing Systems (NIPS) (pp. 577 – 585).\n",
      "7. Van ěk, J., Zelinka, J., Soutner, D., & Psutka, J. (2017). A regularization\n",
      "post layer: An additional way how to make deep neural networks\n",
      "robust. In International Conference on Sta tistical Language and Speech\n",
      "Processing (ICASSP) .\n",
      "8. Hannun, A., Case, C., Casper, J., et al. (2014). Deep speech: Scaling up end-\n",
      "to-end speech recognition. arXiv preprint arXiv, 1412 , 5567.\n",
      "9. Chan, W., Jaitly, N., Le, Q., & Vinyals, O. (2016). Listen, attend and spell: A\n",
      "neural network for large vocabulary conversational speech recognition. In\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 30 ---\n",
      "\n",
      "to-end speech recognition. arXiv preprint arXiv, 1412 , 5567.\n",
      "9. Chan, W., Jaitly, N., Le, Q., & Vinyals, O. (2016). Listen, attend and spell: A\n",
      "neural network for large vocabulary conversational speech recognition. In\n",
      "IEEE International Conference on Acoustics, Speech and Signal Processing\n",
      "(ICASSP) (pp. 4960 – 4964).\n",
      "10. Chiu, C. C., Sainath, T. N., Wu, Y., Prabhavalkar, R., Nguyen, P., Chen, Z.,\n",
      "Kannan, A., Weiss, R. J., Rao, K., Gonina, K., et al. (2018). State-of-the-art\n",
      "speech recognition with sequence-to-sequence models. arXiv preprint arXiv,\n",
      "1712, 01769.\n",
      "11. Zhang, Y., Chan, W., & Jaitly, N. (2017). Very deep convolutional networks for\n",
      "end-to-end speech recognition. In IEEE International Conference on Acoustics,\n",
      "Speech and Signal Processing (ICASSP) .\n",
      "12. Sercu, T., & Goel, V. (2016). Dense prediction on sequences with time-dilated\n",
      "convolutions for speech recognition. arXiv preprint arXiv, 1611 , 09288.\n",
      "13. Wang, Y., Deng, X., Pu, S., & Huang, Z. (2017). Residual convolutional CTC\n",
      "networks for automatic speech recognition. arXiv preprint arXiv, 1702 , 07793.\n",
      "14. Lu, L., Kong, L., Dyer, C., & Smith, N. A. (2017). Multi-task learning with CTC\n",
      "and segmental CRF for speech recognition. arXiv preprint arXiv, 1702 , 06378.\n",
      "15. Kim, S., Hori, T., & Watanabe, S. (2017). Joint CTC-attention based end-to-end\n",
      "speech recognition using multi-task learning. In IEEE International Conference\n",
      "on Acoustics, Speech and Signal Processing (ICASSP) .\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 31 ---\n",
      "\n",
      "15. Kim, S., Hori, T., & Watanabe, S. (2017). Joint CTC-attention based end-to-end\n",
      "speech recognition using multi-task learning. In IEEE International Conference\n",
      "on Acoustics, Speech and Signal Processing (ICASSP) .\n",
      "16. Huang, J. T., Li, J., Yu, D., Deng, L., Gong, Y., et al. (2013). Cross-language\n",
      "knowledge transfer using multilingual deep neural network with shared\n",
      "hidden layers. In IEEE International Conference on Acoustics, Speech and\n",
      "Signal Processing (ICASSP) .\n",
      "17. Deng, J., Xia, R., Zhang, Z., Liu, Y., & Schuller, B. (2014). Introducing shared-\n",
      "hidden-layer autoencoders for transfer learning and their application in\n",
      "acoustic emotion recognition. In IEEE International Conference on Acoustics,\n",
      "Speech and Signal Processing (ICASSP) .\n",
      "18. Hori, T., Watanabe, S., Zhang, Y., & Chan, W. (2017). Advances in joint CTC-\n",
      "attention based end-to-end speech recognition with a deep CNN encoder\n",
      "and RNN-LM. arXiv preprint arXiv, 1706 , 02737.\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 8 of 9\n",
      "19. Van Den Oord, A., Dieleman, S., Zen, H., et al. (2016). Wavenet: A generative\n",
      "model for raw audio. arXiv preprint arXiv, 1609 , 03499.\n",
      "20. Qin, C., & Zhang, L. (2016). Deep neural network based feature extraction\n",
      "using convex-nonnegative matrix factorization for low-resource speech\n",
      "recognition. In Information Technology, Networking, Electronic and\n",
      "Automation Control Conference (ITNEC) .\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 32 ---\n",
      "\n",
      "20. Qin, C., & Zhang, L. (2016). Deep neural network based feature extraction\n",
      "using convex-nonnegative matrix factorization for low-resource speech\n",
      "recognition. In Information Technology, Networking, Electronic and\n",
      "Automation Control Conference (ITNEC) .\n",
      "21. Wall, M. E., Rechtsteiner, A., & Rocha, L. M. (2003). Singular value\n",
      "decomposition and principal component analysis. In A practical approach to\n",
      "microarray data analysis (pp. 91 – 109). Boston, MA: Springer.\n",
      "22. Xue, J., Li, J., & Gong, Y. (2013). Restructuring of deep neural network\n",
      "acoustic models with singular value decomposition. In Interspeech (pp.\n",
      "2365– 2369).\n",
      "23. Ding, C. H., Li, T., & Jordan, M. I. (2010). Convex and semi-nonnegative\n",
      "matrix factorizations. IEEE transactions on pattern analysis and machine\n",
      "intelligence, 32 (1), 45 – 55.\n",
      "24. T. Hori, S. Watanabe, and J. Hershey, “Joint CTC/attention decoding for end-\n",
      "to-end speech recognition, ” Proceedings of the 55th Annual Meeting of the\n",
      "Association for Computational Linguistics . Vol. 1. 2017.\n",
      "25. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.\n",
      "arXiv preprint arXiv, 1412 , 6980.\n",
      "26. Pascanu, R., Mikolov, T., & Bengio, Y. (2012). On the difficulty of training\n",
      "recurrent neural networks. arXiv preprint arXiv, 1211 , 5063.\n",
      "27. Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv\n",
      "preprint arXiv, 1212 , 5701.\n",
      "28. Bisani, M., & Ney, H. (2008). Joint-sequence models for grapheme-to-\n",
      "phoneme conversion. Speech Comm., 50 (5), 434 – 451.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 33 ---\n",
      "\n",
      "27. Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv\n",
      "preprint arXiv, 1212 , 5701.\n",
      "28. Bisani, M., & Ney, H. (2008). Joint-sequence models for grapheme-to-\n",
      "phoneme conversion. Speech Comm., 50 (5), 434 – 451.\n",
      "29. R. L. Weide, “The CMU pronouncing dictionary, ” URL: http://www.speech.cs.\n",
      "cmu.edu/cgi-bin/cmudict, 1998.\n",
      "30. Tóth, L. (2015). Phone recognition with hierarchical convolutional deep\n",
      "maxout networks. EURASIP Journal on Audio, Speech, and Music\n",
      "Processing, vol. 2015, no. 1, pp. 1 – 13.\n",
      "31. Graves, A., Mohamed, A. R., & Hinton, G. (2013). Speech recognition with\n",
      "deep recurrent neural networks. In IEEE International Conference on\n",
      "Acoustics, Speech and Signal Processing (ICASSP) (pp. 6645 – 6649).\n",
      "32. Zhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C. L. Y., & Courville, A.\n",
      "(2017). Towards end-to-end speech recognition with deep convolutional\n",
      "neural networks. arXiv preprint arXiv, 1701 , 02720.\n",
      "33. Zeghidour, N., Usunier, N., Kokkinos, I., Schatz, T., Synnaeve, G., & Dupoux, E.\n",
      "(2017). Learning Filterbanks from Raw Speech for Phone Recognition. arXiv\n",
      "preprint arXiv, 1711 , 01161.\n",
      "Qin et al. EURASIP Journal on Audio, Speech, and Music Processing         (2018) 2018:18 Page 9 of 9\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "dataframe                                                chunks\n",
      "0   RESEARCH Open Access\\nTowards end-to-end speec...\n",
      "1   itional acoustic model is trained in a frame-w...\n",
      "2   benefits in high-resource conditions. A key re...\n",
      "3   Center, Zhengzhou, China\\nFull list of author ...\n",
      "4   the way of data-driven training without expert...\n",
      "5   DNNs are trained in a phone level.\\nOur paper ...\n",
      "6   2.1 Multilingual training with maxout and drop...\n",
      "7   dropout hidden structure without a bottleneck ...\n",
      "8   layer which does not have too many units. Besi...\n",
      "9   processing.\\n2.2 High-level feature extraction...\n",
      "10  through ignoring some component from matrices ...\n",
      "11  Assuming the target weight matrix X has a size...\n",
      "12  ing, the base matrix F and the coefficient mat...\n",
      "13  tonic constraint from CTC to the target attent...\n",
      "14  X\\nπ∈Φ S\\n0\\nðÞ\\nYT\\nt¼1\\np πtjXðÞ\\nð8Þ\\nFor t...\n",
      "15  and attention:\\nℒt ¼ λℒCTC þ 1−λðÞ ℒattention\\...\n",
      "16  Note that although joint CTC-attention models ...\n",
      "17  their delta and delta-delta features are conca...\n",
      "18  We then evaluate on our proposed methods. Firs...\n",
      "19  respectively under the help of CMU dictionary ...\n",
      "20  8 epochs and after which is decayed by half wh...\n",
      "21  training. The CNMF-based features are extracte...\n",
      "22  a limited resource condition in which purely d...\n",
      "23  enced results but are still no better than bes...\n",
      "24  each layer and is trained using all transcript...\n",
      "25  dropout on BLSTM layers. We believe that our m...\n",
      "26  Funding\\nThis work was supported in part by th...\n",
      "27  Center, Zhengzhou, China, in 2005. She is an A...\n",
      "28  2. Chorowski, J., Bahdanau, D., Cho, K., & Ben...\n",
      "29  to-end speech recognition. arXiv preprint arXi...\n",
      "30  15. Kim, S., Hori, T., & Watanabe, S. (2017). ...\n",
      "31  20. Qin, C., & Zhang, L. (2016). Deep neural n...\n",
      "32  27. Zeiler, M. D. (2012). Adadelta: an adaptiv...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TextChunker:\n",
    "\n",
    "    def process(self, pdf_text: str):\n",
    "        \n",
    "        # text_splitter takes strings or a list and returns them in chunks \n",
    "     \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1512,\n",
    "            chunk_overlap=256, \n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_text(pdf_text)\n",
    "        print(type(chunks))\n",
    "        for c in chunks:\n",
    "            print(type(c))\n",
    "        df = pd.DataFrame(chunks, columns=['chunks'])\n",
    "        print(\"Formatted Chunks of Text:\\n\")\n",
    "        for idx, chunk in enumerate(chunks, 1):\n",
    "            print(f\"--- Chunk {idx} ---\\n\")\n",
    "            print(chunk)\n",
    "            print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "        return df\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pdf_path = \"/home/akash/Downloads/paper-32.pdf\"    \n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = []\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page.page_content)\n",
    "        \n",
    "    pages_text = \"\\n\".join(pages)\n",
    "\n",
    "    chunker = TextChunker()\n",
    "    result = chunker.process(pages_text)\n",
    "    \n",
    "    print(\"dataframe\", result)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
